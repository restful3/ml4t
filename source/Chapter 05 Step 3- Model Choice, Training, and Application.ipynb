{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 5 Step 3: 모델 선택, 학습, 그리고 적용**\n",
    "\n",
    "빠르게 진화하는 금융 트레이딩 세계에서, 고급 인공지능(AI) 기법을 활용하면 예측 정확도와 전략적 의사결정을 크게 향상시킬 수 있습니다.\n",
    "\n",
    "AI에서 모델이란, 데이터셋을 기반으로 학습되어 패턴을 인식하고, 예측을 하며, 추천을 제공하거나 다른 작업을 수행하는 수학적 구조입니다. 모델은 학습 데이터셋 내에 존재하는 관계와 구조를 학습하여 해당 데이터셋에 대해, 또는 모델이 보지 못한 데이터에 대해 예측하는 데 사용됩니다.\n",
    "\n",
    "데이터를 분석하기 위한 머신러닝 기법은 크게 감독 학습(supervised learning)과 비감독 학습(unsupervised learning) 두 가지로 나뉩니다. 감독 학습은 입력 특성과 이에 해당하는 출력 라벨이 알려진 라벨된 데이터를 학습시킴으로써 모델을 훈련하는 방식입니다. 이는 분류(classification)와 회귀(regression) 작업에 일반적으로 사용되며, 새로운 보지 못한 데이터에 대해 라벨(분류) 또는 연속적인 값(회귀)을 예측하는 것이 목적입니다. 일반적인 알고리즘으로는 선형 회귀(linear regression), 결정 트리(decision trees), 서포트 벡터 머신(support vector machines), 감독 순위 알고리즘(supervised ranking algorithms) 등이 있습니다. 반면, 비감독 학습은 라벨이 없는 데이터를 다루며, 데이터 내 숨겨진 패턴이나 구조를 발견하는 데 집중합니다. 클러스터링(clustering)과 차원 축소(dimensionality reduction)이 대표적인 응용 분야로, k-평균 클러스터링(k-means clustering)과 주성분 분석(principal component analysis, PCA) 등의 알고리즘이 사용됩니다.\n",
    "\n",
    "대형 언어 모델은 감독 학습과 비감독 학습 기법을 모두 활용하므로, 때때로 반감독 학습(semi-supervised learning)이라고도 불립니다. 먼저 모델은 명시적인 라벨 없이 방대한 텍스트 데이터를 기반으로 문장 내 다음 단어를 예측하는 방식으로 학습됩니다. 이후, 특정 라벨이 달린 문서 집합을 사용하여 미세 조정(fine-tuning)됩니다.\n",
    "\n",
    "강화 학습(reinforcement learning)은 감독/비감독 학습과는 별개의 개념이며, 별도의 장에서 설명됩니다. 강화 학습은 에이전트가 환경 내에서 행동을 취하면서 누적 보상을 극대화하기 위해 시행착오를 통해 의사결정을 학습하는 패러다임입니다. 이 모델의 예로는 자율주행 자동차가 있습니다.\n",
    "\n",
    "이 장에서는 회귀 기법 및 머신러닝 알고리즘을 소개하며, 이는 현대 금융 분석가와 트레이더에게 필수적인 도구가 되었습니다.\n",
    "\n",
    "우리는 선형 회귀(linear regression)부터 시작합니다. 이는 종속 변수와 하나 이상의 독립 변수 간의 관계를 모델링하는 기본적인 방법으로, 과거 데이터를 기반으로 자산 가격을 예측하는 데 핵심적입니다. 다항 회귀(polynomial regression)는 비선형 관계를 적합시킴으로써 더 복잡한 추세를 포착할 수 있도록 확장된 형태입니다. 최소 절대 수축 및 선택 연산자(LASSO) 회귀는 과적합을 방지하기 위해 정규화를 도입하며, 많은 수의 예측 변수를 포함하는 데이터셋을 효과적으로 처리할 수 있습니다. 리지 회귀(ridge regression)도 유사하게 패널티 항을 추가하여 모델의 일반화를 향상시킵니다. 마르코프 전환 동적 회귀(Markov switching dynamic regression)는 서로 다른 체제로 전환하는 시계열 데이터를 모델링하며, 시장 조건을 보다 정확하게 반영합니다. 결정 트리 회귀(decision tree regression)는 비모수(non-parametric) 방식으로 데이터를 분할하고 결정 규칙에 따라 예측을 수행합니다. 웨이브렛 예측과 결합된 서포트 벡터 머신(SVM) 회귀는, 잡음이 많은 환경에서도 정밀한 예측을 가능하게 합니다. 다중 클래스 랜덤 포레스트 모델(multiclass random forest model)은 두 개 이상의 범주를 포함한 분류 문제에 효과적으로 대응하기 위해 랜덤 포레스트 알고리즘을 확장한 것입니다. 로지스틱 회귀(logistic regression)는 시장 이벤트의 발생 가능성을 예측하는 분류 작업에 핵심적입니다. 회귀를 넘어서, 은닉 마르코프 모델(hidden Markov models)은 숨겨진 상태를 가진 시계열 데이터를 모델링하는 데 강력한 프레임워크를 제공하며, 시장 체제 전환을 포착할 수 있습니다. 가우시안 나이브 베이즈(Gaussian Naive Bayes)는 분류 작업에 확률적 방법을 적용하며, 특히 고차원 데이터셋에서 유용합니다. 마지막으로 합성곱 신경망(convolutional neural networks, CNN)은 원래 이미지 처리에 사용되지만, 금융 시계열에 적용되어 복잡한 패턴과 추세를 포착할 수 있습니다.\n",
    "\n",
    "LGBRanker는 항목의 관련성이나 중요도에 따라 순위를 매기는 데 탁월한 성능을 보입니다. OPTICS 클러스터링은 다양한 밀도의 클러스터를 식별할 수 있는 능력으로 대규모 데이터셋 내 구조 발견에 이상적입니다. OpenAI 언어 모델은 고급 자연어 이해 능력을 갖추고 있어 복잡한 언어 작업을 수행하는 데 적합합니다. Amazon Chronos는 시계열 예측을 위해 설계된 머신러닝 기반 모델로, 정확하고 확장 가능한 예측을 제공합니다. FinBERT는 금융 감성 분석에 특화되어 있으며, 금융 텍스트를 해석하여 시장 심리를 평가하고 트레이딩 전략 수립에 도움을 줍니다.\n",
    "\n",
    "각 기법은 금융 트레이딩에서의 실제 응용을 보여주는 Python 예제와 함께 제시되며, 각 모델의 성능을 평가합니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 회귀(Regression)\n",
    "\n",
    "회귀는 변수들이 서로 어떻게 관련되어 있는지를 찾는 통계적 방법입니다. 이는 종속 변수가 독립 변수들에 어떻게 의존하는지를 보여주는 수학적 모델을 데이터셋에 적합시키려 합니다.\n",
    "\n",
    "금융에서 일반적인 종속 변수는 자산 가격이고, 독립 변수는 경제 요인이나 시장 지표일 수 있습니다.\n",
    "\n",
    "**선형 회귀(Linear Regression)**\n",
    "\n",
    "| 설명               | 선형 회귀는 종속 변수와 하나 이상의 독립 변수 간의 관계를 모델링하는 기본적인 방법입니다.           |\n",
    "| :--------------- | :------------------------------------------------------------ |\n",
    "| 금융 분야의 주요 활용 사례  | 추세 분석, 리스크 관리, 주가, 수익률, 금리, 기업 가치 또는 경제 지표와 같은 금융 지표의 향후 값 예측 |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수는 아니지만 권장됨                                                  |\n",
    "| 파이썬 라이브러리 설치     | ```pip install numpy pandas matplotlib scikit-learn```            |\n",
    "\n",
    "선형 회귀는 종속 변수와 하나 이상의 독립 변수 간의 관계를 모델링하는 데 사용되는 기본적인 통계 기법입니다.\n",
    "\n",
    "이 방법의 주요 장점은 계산 속도가 빠르고 구현과 해석이 간단하여 예측 모델링을 시작하기에 적합하다는 점입니다. 하지만 관계가 비선형이거나 데이터에 큰 이상치가 있는 경우에는 성능이 떨어질 수 있습니다.\n",
    "\n",
    "금융에서는 주로 추세 분석, 리스크 관리, 주가, 수익률, 금리, 기업 가치, 경제 지표 등의 미래 값을 예측하는 데 사용됩니다.\n",
    "\n",
    "정규화나 표준화는 선형 회귀에 필수는 아니지만, 특히 특성의 스케일이 다를 경우에는 모델의 성능과 해석력을 향상시키는 데 도움이 됩니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10  # 특성: 0과 10 사이의 임의 값\n",
    "y = 2.5 * X + np.random.randn(100, 1) * 2  # 목표값: 잡음이 포함된 선형 관계\n",
    "\n",
    "# 학습용 및 테스트용 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 선형 회귀 모델 사용\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 학습 데이터, 테스트 데이터, 모델 적합선을 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "plt.plot(X_test, model.predict(X_test), color='red', linewidth=2, label='Fitted line')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "\n",
    "# 결과 해석\n",
    "# 낮은 MSE와 1에 가까운 R^2 점수는 좋은 적합도를 의미합니다.\n",
    "# 더 많은 특성을 추가하거나 기존 특성을 변환하거나 정규화 기법을 사용하여 모델을 개선할 수 있습니다.\n",
    "```\n",
    "\n",
    "출력 (그림 5.1):\n",
    "\n",
    "```\n",
    "Mean Squared Error: 4.173733526278071  \n",
    "R^2 Score: 0.9295849975491168\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_01.png\" width=800>\n",
    "\n",
    "\n",
    "**다항 회귀(Polynomial Regression)**\n",
    "\n",
    "| 설명               | 다항 회귀는 독립 변수와 종속 변수 간의 비선형 관계를 적합시켜 보다 복잡한 추세를 포착할 수 있도록 선형 회귀를 확장한 방법입니다. |\n",
    "| :--------------- | :------------------------------------------------------------------------- |\n",
    "| 금융 분야의 주요 활용 사례  | 금융 시장의 순환적 행동 모델링, 금리 예측, 경제 성장 모델의 적합도 향상, 금융 자산 성과 분석                    |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수는 아니지만 권장됨                                                               |\n",
    "| 파이썬 라이브러리 설치     | `pip install numpy pandas matplotlib scikit-learn`                         |\n",
    "\n",
    "다항 회귀는 선형 회귀의 일반화 형태로, 종속 변수와 독립 변수 간의 곡선 형태의 관계를 허용합니다. 변수 간의 관계가 다항식 항으로 모델링될 수 있는 비선형일 때 다항 회귀를 사용합니다.\n",
    "\n",
    "고차 다항식에서는 과적합(overfitting)에 주의해야 합니다. 과적합을 방지하기 위해 정규화 기법이 사용되며, 이는 손실 함수에 패널티 항을 추가하여 다항식 방정식의 계수가 과도하게 커지는 것을 억제합니다. 일반적인 정규화 유형은 두 가지로, 계수의 절대값을 더하는 L1 정규화(이후 설명할 LASSO 회귀로 이어짐)와 계수의 제곱 값을 더하는 L2 정규화(이후 설명할 리지 회귀로 이어짐)가 있습니다. 정규화는 훈련 데이터에 너무 밀착되어 잡음을 학습하는 문제를 피하고, 보지 못한 데이터에 대한 성능을 향상시키는 데 도움을 줍니다.\n",
    "\n",
    "금융 분야에서는 다항 회귀를 통해 시계열 데이터에서 더 복잡한 관계를 모델링하거나, 경제 지표의 순환적 행동을 포착하며, 주가 예측 모델의 적합도를 개선하는 데 사용합니다.\n",
    "\n",
    "특히 다항 항이 특성 값의 범위를 크게 증가시키는 경우, 정규화 또는 표준화는 유용할 수 있습니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10  # 특성: 0과 10 사이의 임의 값\n",
    "y = 2.5 * X**2 + np.random.randn(100, 1) * 10  # 목표값: 잡음이 포함된 이차 관계\n",
    "\n",
    "# 학습용 및 테스트용 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 다항 특성과 선형 회귀 모델 사용\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_poly, y_train)\n",
    "\n",
    "# 학습 데이터, 테스트 데이터, 모델 적합 곡선 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_plot = model.predict(poly.transform(X_plot))\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Fitted curve')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "y_pred = model.predict(X_test_poly)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "```\n",
    "\n",
    "출력 (그림 5.2):\n",
    "\n",
    "```\n",
    "Mean Squared Error: 102.85467801527334\n",
    "R^2 Score: 0.981491014534497\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_02.png\" width=800>\n",
    "\n",
    "**LASSO 회귀(LASSO Regression)**\n",
    "\n",
    "| 설명               | LASSO 회귀는 선형 회귀에 정규화를 도입하여 과적합을 방지하고, 많은 수의 예측 변수를 포함하는 데이터셋을 효과적으로 처리합니다. |\n",
    "| :--------------- | :------------------------------------------------------------------------- |\n",
    "| 금융 분야의 주요 활용 사례  | 금융 모델의 특성 선택, 신용 위험 예측, 주가에 영향을 미치는 주요 요인 식별, 포트폴리오 최적화 모델 향상              |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수                                                                         |\n",
    "| 파이썬 라이브러리 설치     | `pip install numpy pandas matplotlib scikit-learn`                         |\n",
    "\n",
    "최소 절대 수축 및 선택 연산자(LASSO) 회귀는 덜 중요한 특성의 계수를 0으로 수축시키는 정규화 항을 포함한 선형 회귀의 한 형태입니다. 많은 특성이 있을 때, 그 중 일부만이 목표 변수 예측에 중요하다고 판단되는 경우 LASSO 회귀는 좋은 선택입니다. 이는 과적합을 피하고 모델의 일반화를 향상시킵니다. 그러나 모든 특성이 중요하거나 모든 특성을 포함하는 정확한 모델이 필요한 경우에는 성능이 떨어질 수 있습니다.\n",
    "\n",
    "<img src=\"./images/fig_05_03.png\" width=800>\n",
    "\n",
    "LASSO 회귀에서는 매개변수 $\\alpha$가 정규화 강도를 제어합니다. $\\alpha$가 클수록 정규화가 강해지며, 중요하지 않은 특성들의 계수가 0에 가까워져 효과적으로 특성 선택을 수행하게 됩니다.\n",
    "\n",
    "형식적으로, 손실 함수는 다음과 같이 계수의 절댓값 합계를 패널티 항으로 포함하도록 수정됩니다:\n",
    "\n",
    "$\\operatorname{Loss} = RSS + \\alpha \\sum_{i=1}^{n} \\left| w_i \\right|$\n",
    "\n",
    "잔차 제곱합(RSS)은 관측값과 예측값 간의 제곱 차의 합이며, $w\\_i$는 모델 계수, $\\alpha$는 정규화 매개변수입니다. LASSO 회귀는 이 손실 함수를 최소화하는 것을 목표로 하며, $\\alpha$가 클수록 계수는 더 작아집니다.\n",
    "\n",
    "금융 분야에서는, 자산 수익률이나 신용 위험과 같은 금융 결과를 예측하는 데 있어 가장 중요한 예측 변수를 식별하는 데 LASSO 회귀가 도움을 줄 수 있습니다.\n",
    "\n",
    "정규화 항이 모든 특성에 균등하게 적용되도록 하기 위해, LASSO 회귀에서는 정규화 또는 표준화가 필수적입니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)  # 특성: 10개의 임의 값\n",
    "true_coeffs = np.array([22.5, -1.5, 0, 100, 3, 0, 45, 0, 1, 0])\n",
    "y = X @ true_coeffs + np.random.randn(100) * 2  # 목표값: 잡음이 포함된 선형 조합\n",
    "\n",
    "# 학습용 및 테스트용 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Lasso 회귀 사용\n",
    "model = Lasso(alpha=0.01)  # 더 나은 적합을 위한 조정된 alpha\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터, 모델 적합 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "plt.scatter(y_train, y_pred_train, color='blue', alpha=0.5, label='Training data')\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "plt.scatter(y_test, y_pred_test, color='green', alpha=0.5, label='Test data')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Ideal fit')\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Lasso Regression: Predicted vs. Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 회귀 계수 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.coef_, marker='o', linestyle='none', label='Lasso coefficients')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('Lasso Regression Coefficients')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "mse = mean_squared_error(y_test, y_pred_test)\n",
    "r2 = r2_score(y_test, y_pred_test)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "```\n",
    "\n",
    "출력 (그림 5.3 및 5.4):\n",
    "\n",
    "```\n",
    "Mean Squared Error: 2.885180122300261\n",
    "R^2 Score: 0.9961485834952417\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_04.png\" width=800>\n",
    "\n",
    "대부분의 특성 계수가 0에 가까운 것을 확인할 수 있습니다.\n",
    "\n",
    "**리지 회귀(Ridge Regression)**\n",
    "\n",
    "| 설명               | 리지 회귀는 손실 함수에 계수의 제곱합에 비례하는 패널티를 추가하여 선형 회귀를 확장한 것으로, 모든 특성을 모델에 유지하면서 계수를 0에 가깝게 수축시켜 과적합을 방지합니다. |\n",
    "| :--------------- | :------------------------------------------------------------------------------------------------- |\n",
    "| 금융 분야의 주요 활용 사례  | 경제 지표 예측, 주식 수익률 예측, 포트폴리오 최적화에서의 모델 복잡도 감소, 금융 데이터에서 다중공선성 문제 처리                                  |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수                                                                                                 |\n",
    "| 파이썬 라이브러리 설치     | `pip install numpy pandas matplotlib scikit-learn`                                                 |\n",
    "\n",
    "리지 회귀는 독립 변수들 간의 상관관계가 높은 다중공선성 문제가 있는 다중 회귀 데이터를 분석하는 데 사용되는 기법입니다. 다중공선성은 독립 변수들이 서로 강하게 연관되어 있어 각 변수의 종속 변수에 대한 개별적인 영향을 분리하기 어렵게 만들며, 이로 인해 계수 추정치가 신뢰할 수 없게 됩니다. 리지 회귀는 회귀 추정치에 일정한 편향을 도입하여 모델의 예측 정확도와 해석력을 향상시킬 수 있습니다.\n",
    "\n",
    "리지 회귀에서는 정규화 매개변수 또는 패널티 항인 $\\alpha$가 모델에 적용되는 정규화의 정도를 제어합니다. 이는 훈련 데이터에 잘 맞는 것과 모델 계수를 작게 유지하여 과적합을 방지하는 것 사이의 균형을 조절합니다. 리지 회귀 손실 함수의 수식은 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = RSS + \\alpha \\sum_{i=1}^{n} w_i^2\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "\n",
    "* RSS는 관측값과 예측값 간의 제곱 차 합을 측정합니다.\n",
    "* $\\alpha$는 정규화 매개변수입니다. 0이면 일반적인 선형 회귀가 되며, 값이 클수록 계수는 더 많이 수축되어 과소적합의 위험이 있습니다.\n",
    "* $w_i$는 모델의 계수입니다.\n",
    "\n",
    "금융에서는 예측 성능을 유지하면서 모델의 복잡도를 줄이기 위해 주로 예측과 모델 단순화를 위해 사용됩니다.\n",
    "\n",
    "정규화 항이 모든 특성에 균일하게 적용되도록 보장하기 위해 정규화 또는 표준화는 필수입니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)  # 특성: 10개의 임의 값\n",
    "true_coeffs = np.array([20.5, -1.5, 0, 45, 3, 0, 0, 0, 1, 0])\n",
    "y = X @ true_coeffs + np.random.randn(100) * 2  # 목표값: 잡음이 포함된 선형 조합\n",
    "\n",
    "# 학습용 및 테스트용 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Ridge 회귀 사용\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터, 모델 적합 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "y_pred_train = model.predict(X_train_scaled)\n",
    "plt.scatter(y_train, y_pred_train, color='blue', alpha=0.5, label='Training data')\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "plt.scatter(y_test, y_pred_test, color='green', alpha=0.5, label='Test data')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2, label='Ideal fit')\n",
    "plt.xlabel('Actual values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('Ridge Regression: Predicted vs. Actual')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 고차원 데이터에서는 회귀 계수에 집중\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(model.coef_, marker='o', linestyle='none', label='Ridge coefficients')\n",
    "plt.xlabel('Feature index')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.title('Ridge Regression Coefficients')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "```\n",
    "\n",
    "**정규화 매개변수(alpha)는 수축 정도를 조절하는 데 사용됩니다.**\n",
    "\n",
    "출력 (그림 5.5 및 5.6):\n",
    "\n",
    "```\n",
    "Mean Squared Error: 2.9934174282331862\n",
    "R^2 Score: 0.982163488280449\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_05.png\" width=800>  \n",
    "\n",
    "<img src=\"./images/fig_05_06.png\" width=800>\n",
    "\n",
    "**마르코프 전환 동적 회귀(Markov Switching Dynamic Regression)**\n",
    "\n",
    "| 설명               | 마르코프 전환 동적 회귀는 서로 다른 체제로 전환되는 시계열 데이터를 모델링하여 시장 상황을 보다 정확하게 반영합니다. |\n",
    "| :--------------- | :----------------------------------------------------------------- |\n",
    "| 금융 분야의 주요 활용 사례  | 경기 순환 및 경제 성장 모델링, 강세장과 약세장 체제 탐지, 구조적 변화가 있는 예측, 리스크 관리 모델 향상     |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수는 아니지만 권장됨                                                       |\n",
    "| 파이썬 라이브러리 설치     | `pip install numpy pandas matplotlib statsmodels`                  |\n",
    "\n",
    "마르코프(또는 마르코프 전환) 회귀 모델은 변수의 효과가 숨겨진, 종종 관측되지 않는 상태에 따라 달라질 수 있는 복잡한 시계열 패턴을 모델링하는 데 유용합니다. 이 모델들에서는 여러 개의 가능한 상태(\"체제\")가 존재하며, 각 상태는 자체 회귀 계수 집합을 가집니다(예: 상승장과 하락장에서 서로 다른 계수). 상태 간 전환은 무작위 과정에 의해 이루어지며, 각 상태에만 의존하고 과거 이력에는 의존하지 않습니다.\n",
    "\n",
    "마르코프 전환 동적 회귀(MSDR) 모델은 시계열 데이터에서 체제 변화를 포착합니다. 이들은 회귀 모델의 계수가 마르코프 과정에 따라 서로 다른 상태로 전환될 수 있도록 허용합니다. 이 모델은 데이터셋이 작거나 체제 변화가 드물거나 무작위일 경우에는 적합하지 않습니다.\n",
    "\n",
    "금융에서는 MSDR 모델이 경제 순환을 모델링하거나 시장 체제를 탐지하며, 구조적 변화 및 체제 전환을 고려함으로써 예측을 향상시키는 데 유용합니다.\n",
    "\n",
    "여러 특성을 다룰 경우 수렴성과 해석력을 향상시키기 위해 정규화 또는 표준화를 권장합니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "\n",
    "# 서로 다른 변동성을 가진 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "X = np.linspace(0, 20, n)\n",
    "regime_1 = 2.5 * X[:n//2] + np.random.randn(n//2) * 5  # 체제 1에서 높은 변동성\n",
    "regime_2 = -1.5 * X[n//2:] + 30 + np.random.randn(n//2) * 1  # 체제 2에서 낮은 변동성\n",
    "y = np.concatenate([regime_1, regime_2])\n",
    "\n",
    "# 데이터프레임 생성\n",
    "data = pd.DataFrame({'X': X, 'y': y})\n",
    "\n",
    "# 마르코프 전환 동적 회귀 적합\n",
    "model = MarkovRegression(data['y'], k_regimes=2, trend='c', switching_variance=True)\n",
    "result = model.fit()\n",
    "\n",
    "# 관측 데이터와 적합된 체제 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(data['X'], data['y'], label='Observed', color='blue')\n",
    "smoothed_probs = result.smoothed_marginal_probabilities\n",
    "\n",
    "for t in range(len(smoothed_probs)):\n",
    "    if smoothed_probs.iloc[t, 0] > 0.5:\n",
    "        plt.plot(data['X'].iloc[t], data['y'].iloc[t], 'ro', alpha=0.5)\n",
    "    else:\n",
    "        plt.plot(data['X'].iloc[t], data['y'].iloc[t], 'go', alpha=0.5)\n",
    "\n",
    "regime_changes = np.argmax(smoothed_probs.values, axis=1)\n",
    "for i in range(1, len(regime_changes)):\n",
    "    if regime_changes[i] != regime_changes[i - 1]:\n",
    "        plt.axvline(x=data['X'].iloc[i], color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Target (y)')\n",
    "plt.title('Markov Switching Dynamic Regression with Two Regimes')\n",
    "plt.legend(['Observed', 'Regime 1', 'Regime 2'])\n",
    "plt.show()\n",
    "\n",
    "# 체제 확률 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data['X'], smoothed_probs[0], label='Regime 1 Probability', color='red')\n",
    "plt.plot(data['X'], smoothed_probs[1], label='Regime 2 Probability', color='green')\n",
    "plt.xlabel('Feature (X)')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Smoothed Regime Probabilities')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 출력\n",
    "print(result.summary())\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_extra.png\" width=800>  \n",
    "\n",
    "출력 (그림 5.7 및 5.8):\n",
    "\n",
    "<img src=\"./images/fig_05_07.png\" width=800>  \n",
    "\n",
    "<img src=\"./images/fig_05_08.png\" width=800>\n",
    "\n",
    "\n",
    "**결정 트리 회귀(Decision Tree Regression)**\n",
    "\n",
    "| 설명              | 결정 트리 회귀는 데이터를 분기 규칙에 따라 분할하여 예측을 수행하는 비모수(non-parametric) 접근 방식을 제공합니다. |\n",
    "| :-------------- | :----------------------------------------------------------------------- |\n",
    "| 금융 분야의 주요 활용 사례 | 신용 위험 모델링, 재무적 위기 예측, 주가 움직임의 주요 요인 식별, 대출 부도 예측                         |\n",
    "| 특성              | 필요 없음                                                                    |\n",
    "| 정규화/표준화         |                                                                          |\n",
    "| 필요 라이브러리        | `pip install numpy pandas matplotlib scikit-learn`                       |\n",
    "\n",
    "결정 트리 회귀는 특성 값에 따라 데이터를 여러 하위 집합으로 분할하는 비선형 예측 모델 기법으로, 결과적으로 트리 형태의 구조를 형성합니다. 복잡한 특성 엔지니어링 없이 비선형 관계를 포착하고, 해석 가능한 모델이 필요한 경우에 유용합니다. 하지만 과적합에 민감하므로, 일반화를 개선하기 위해 가지치기(pruning) 기법이나 랜덤 포레스트와 같은 앙상블 기법이 자주 사용됩니다.\n",
    "\n",
    "금융에서는 결정 트리 회귀를 사용하여 데이터 내의 복잡한 관계를 모델링하거나, 재무 결과에 영향을 주는 주요 결정 요인을 식별하며, 직관적인 의사결정 과정 시각화를 제공합니다.\n",
    "\n",
    "결정 트리 회귀는 특성 스케일에 영향을 받지 않기 때문에 정규화나 표준화가 필요하지 않습니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2.5 * np.sin(X).ravel() + np.random.randn(100) * 0.5\n",
    "\n",
    "# 학습용 및 테스트용 데이터로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 결정 트리 회귀 사용\n",
    "model = DecisionTreeRegressor(max_depth=3)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 학습 데이터, 테스트 데이터, 모델 적합 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "X_plot = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_plot = model.predict(X_plot)\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Fitted model')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Decision Tree Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 생성된 트리 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(model, filled=True)\n",
    "plt.title('Decision Tree Structure')\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "```\n",
    "\n",
    "```\n",
    "Mean Squared Error: 0.573935429046908\n",
    "R^2 Score: 0.8041605163184924\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_09.png\" width=800>  \n",
    "\n",
    "<img src=\"./images/fig_05_10.png\" width=800>\n",
    "\n",
    "\n",
    "**웨이블릿 예측을 활용한 서포트 벡터 머신 회귀(Support Vector Machines Regression with Wavelet Forecasting)**\n",
    "\n",
    "|        설명       | 웨이블릿 예측과 결합된 서포트 벡터 머신(SVM) 회귀는 잡음이 많은 환경에서도 정밀한 예측을 위해 SVM과 웨이블릿 변환의 장점을 결합합니다. |\n",
    "| :-------------: | :------------------------------------------------------------------------------: |\n",
    "| 금융 분야의 주요 활용 사례 |                      주가 예측, 변동성 예측, 고빈도 트레이딩, 순환적 금융 데이터 분석                      |\n",
    "|        특성       |                                        필수                                        |\n",
    "|     정규화/표준화     |                                        필수                                        |\n",
    "|   파이썬 라이브러리 설치  |           `pip install numpy pandas matplotlib scikit-learn pywavelets`          |\n",
    "\n",
    "SVM 회귀는 추세와 계절 패턴이 있는 시계열 예측을 위해 웨이블릿 변환과 함께 사용되는 강력한 방법입니다.\n",
    "\n",
    "SVM은 분류와 회귀 모두에 사용할 수 있는 감독 학습 모델입니다:\n",
    "\n",
    "* 분류 문제에서는 데이터를 최적으로 구분하는 경계(hyperplane)를 찾아 서로 다른 그룹으로 나눌 수 있습니다.\n",
    "* 서포트 벡터 회귀(SVR)는 가능한 한 부드러운 함수로, 주어진 마진 경계 내에 최대한 많은 데이터를 포함하는 함수를 찾는 것이 목적입니다.\n",
    "\n",
    "SVM 회귀는 복잡한 관계를 포착할 수 있으며, 웨이블릿 변환은 시계열 데이터를 다양한 주파수 성분으로 분해합니다. 이 기법은 계산 비용이 높고 단순한 모델보다 해석이 어렵다는 단점이 있습니다.\n",
    "\n",
    "SVM은 금융 예측에서 유용하며, 금융 및 경제 데이터의 비선형 패턴을 다룰 수 있는 능력으로 인해 이상적입니다.\n",
    "\n",
    "특성 스케일이 다를 때에도 적절한 수렴성과 성능을 보장하기 위해 SVM에는 정규화 또는 표준화가 필수입니다.\n",
    "\n",
    "**파이썬 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "X = np.linspace(0, 20, n).reshape(-1, 1)\n",
    "y = 2.5 * np.sin(X).ravel() + np.random.randn(n) * 0.5\n",
    "\n",
    "# 웨이블릿 변환 적용\n",
    "coeffs = pywt.wavedec(y, 'db1', level=2)\n",
    "y_wavelet = pywt.waverec(coeffs, 'db1')\n",
    "\n",
    "# 학습용 및 테스트용 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_wavelet, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM 회귀 사용\n",
    "model = SVR(kernel='rbf', C=100, epsilon=0.01)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 학습 및 테스트 데이터와 모델 적합 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training data')\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "X_plot = np.linspace(0, 20, 200).reshape(-1, 1)\n",
    "X_plot_scaled = scaler.transform(X_plot)\n",
    "y_plot = model.predict(X_plot_scaled)\n",
    "plt.plot(X_plot, y_plot, color='red', linewidth=2, label='Fitted model')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('SVM Regression with Wavelet Forecasting')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합도 통계 확인\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "```\n",
    "\n",
    "```\n",
    "Mean Squared Error: 0.36018129235244006\n",
    "R^2 Score: 0.8918512062597711\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_11.png\" width=800>\n",
    "\n",
    "Figure 5.11 SVM 회귀 예시.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pywt\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "X = np.linspace(0, 20, n).reshape(-1, 1)\n",
    "y = 2.5 * np.sin(X).ravel() + np.random.randn(n) * 0.5\n",
    "\n",
    "# 웨이블릿 변환 적용\n",
    "coeffs = pywt.wavedec(y, 'db1', level=2)\n",
    "y_wavelet = pywt.waverec(coeffs, 'db1')\n",
    "\n",
    "# 학습용 및 테스트용 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_wavelet, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM 회귀와 그리드 서치 사용\n",
    "model = SVR(kernel='rbf')\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'epsilon': [0.01, 0.1, 0.5, 1],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1, 10]\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='r2')\n",
    "\n",
    "# 모델 학습\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 최적의 파라미터 및 점수 출력\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'Best R2 score: {best_score}')\n",
    "\n",
    "# 최적 모델 사용\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 테스트 세트에서 평가\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Test Mean Squared Error: {mse}')\n",
    "print(f'Test R2 Score: {r2}')\n",
    "```\n",
    "\n",
    "```\n",
    "Best parameters: {'C': 10, 'epsilon': 0.5, 'gamma': 10}\n",
    "Best R^2 score: 0.9192064453967035\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "**Multiclass Random Forest Model**\n",
    "\n",
    "| Description                                    | 앙상블 학습 방법 중 하나로, 여러 개의 결정 트리를 구축하여 데이터를 두 개 이상의 범주로 분류하는 기법. |\n",
    "| :--------------------------------------------- | :------------------------------------------------------------------------------------------------------------------ |\n",
    "| Key Use Cases in Finance                       | 사기 탐지, 고객 세분화, 신용 위험 평가.                                                 |\n",
    "| Feature Normalization/Standardization Required | Not required.                                                                                                       |\n",
    "| Python Libraries Setup                         | ```pip install numpy lightgbm scikit-learn matplotlib seaborn```                                                          |\n",
    "\n",
    "다중 클래스 랜덤 포레스트 모델은 둘 이상의 범주로 데이터를 분류하기 위해 여러 개의 결정 트리를 구축하는 앙상블 학습 방법입니다. 단일 결정 트리를 사용하는 것과 비교하여 여러 결정 트리의 예측을 집계함으로써 모델의 정확도를 향상시키고 과적합을 줄입니다.\n",
    "\n",
    "**Key Concepts**\n",
    "\n",
    "* Ensemble Learning: 앙상블 학습은 여러 머신러닝 모델의 예측을 결합하여 하나의 더 정확한 예측을 생성하는 기법으로, 다양한 모델의 강점을 통합함으로써 개별 모델보다 더 나은 성능을 발휘할 수 있다는 동기를 가집니다. 대표적인 앙상블 방법에는 배깅, 부스팅, 스태킹이 있습니다.\n",
    "* Decision Tree: 결정 트리는 입력 특성 값에 따라 데이터를 하위 집합으로 나누는 간단하지만 강력한 예측 모델입니다. 트리의 각 노드는 특성을 나타내고, 각 분기는 결정 규칙을, 각 리프 노드는 클래스 레이블을 나타냅니다. 결정 트리는 해석이 쉽고 수치형 및 범주형 데이터를 모두 처리할 수 있지만 과적합의 위험이 있습니다.\n",
    "* Random Forest: 랜덤 포레스트는 학습 중에 여러 결정 트리를 구축하고 이들의 예측을 결합하여 정확도를 높이고 과적합을 제어하는 앙상블 학습 방법입니다. 부트스트래핑(복원 추출 샘플링) 및 특성 무작위화를 사용하여 트리 간의 다양성과 비상관성을 보장하고, 보다 일반화된 강력한 모델을 만듭니다.\n",
    "* Bagging: 배깅(Bootstrap Aggregating의 줄임말)은 데이터의 서로 다른 무작위 하위 집합을 사용하여 여러 모델을 학습시키고, 이들의 예측을 평균 내는 앙상블 학습 기법입니다.\n",
    "\n",
    "**Python Example**\n",
    "\n",
    "우리는 대규모 데이터셋 분류에서 뛰어난 성능을 보이는 LightGBM Python 라이브러리를 선택했습니다. 이 라이브러리의 하이퍼파라미터는 다음과 같습니다:\n",
    "\n",
    "* boosting\\_type: 사용할 부스팅 알고리즘입니다. 본 예제에서는 'rf' 부스팅 타입(랜덤 포레스트)을 사용하며, 이는 다중 클래스 분류를 위한 LightGBM 모델로, 여러 트리를 결합하여 다중 클래스 로그 손실을 최소화함으로써 성능을 향상시킵니다.\n",
    "* num\\_leaves: 하나의 트리에서 최대 리프 수입니다. 값이 클수록 모델이 과적합할 가능성이 있습니다.\n",
    "* learning\\_rate: 가중치 변경에 사용되는 스텝 사이즈의 감소폭입니다. 값이 작을수록 모델이 더 견고해지며, 더 많은 트리를 필요로 합니다.\n",
    "* feature\\_fraction: 트리 구축 시 각 분할마다 고려할 특성의 비율입니다.\n",
    "* bagging\\_fraction: 각 부스팅 반복에서 사용할 데이터의 비율입니다.\n",
    "* bagging\\_freq: 배깅이 수행되는 반복 주기입니다.\n",
    "\n",
    "우리는 sklearn의 make\\_classification 메서드를 사용하여 샘플 데이터를 생성하고, GridSearchCV를 사용하여 최적의 하이퍼파라미터 값을 찾습니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# make_classification을 사용하여 적절한 샘플 데이터 생성\n",
    "n_samples = 1000\n",
    "n_features = 5\n",
    "n_classes = 3\n",
    "X, y = make_classification(\n",
    "    n_samples=n_samples,\n",
    "    n_features=n_features,\n",
    "    n_informative=int(n_features * 0.6),\n",
    "    n_redundant=n_features - int(n_features * 0.6),\n",
    "    n_classes=n_classes,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# 학습용 및 테스트용 데이터셋으로 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋 내 클래스 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=y)\n",
    "plt.title(\"Distribution of Classes in the Dataset\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "# 하이퍼파라미터 튜닝을 위한 파라미터 그리드 정의\n",
    "param_grid = {\n",
    "    'num_leaves': [30, 50, 70],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'feature_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'bagging_fraction': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'bagging_freq': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# LightGBM 모델 생성\n",
    "lgb_estimator = lgb.LGBMClassifier(\n",
    "    boosting_type='rf',\n",
    "    objective='multiclass',\n",
    "    num_class=n_classes,\n",
    "    metric='multi_logloss'\n",
    ")\n",
    "\n",
    "# 그리드 서치 수행\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgb_estimator,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 그리드 서치에서 최적의 파라미터와 정확도 획득\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Accuracy:\", best_accuracy)\n",
    "\n",
    "# 최적의 파라미터로 모델 학습\n",
    "model = lgb.LGBMClassifier(\n",
    "    boosting_type='rf',\n",
    "    objective='multiclass',\n",
    "    num_class=n_classes,\n",
    "    **best_params\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "# 모델 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy after tuning:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 혼동 행렬\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    conf_matrix,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=range(n_classes),\n",
    "    yticklabels=range(n_classes)\n",
    ")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Class\")\n",
    "plt.ylabel(\"True Class\")\n",
    "plt.show()\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "lgb.plot_importance(model, max_num_features=10, importance_type=\"gain\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과 (그림 5.12, 5.13, 5.14)\n",
    "\n",
    "<img src=\"./images/fig_05_12.png\" width=800>\n",
    "\n",
    "<img src=\"./images/fig_05_13.png\" width=800>\n",
    "\n",
    "<img src=\"./images/fig_05_14.png\" width=800>\n",
    "\n",
    "튜닝 후 정확도: 0.875\n",
    "Classification Report:\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "           0       0.88      0.89      0.88        56\n",
    "           1       0.90      0.81      0.85        74\n",
    "           2       0.86      0.93      0.89        70\n",
    "\n",
    "    accuracy                           0.88       200\n",
    "   macro avg       0.88      0.88      0.88       200\n",
    "weighted avg       0.88      0.88      0.87       200\n",
    "```\n",
    "\n",
    "**로지스틱 회귀**\n",
    "\n",
    "| 설명               | 로지스틱 회귀는 시장 이벤트의 가능성을 예측하는 분류 작업에 핵심적인 역할을 합니다.  |\n",
    "| :--------------- | :----------------------------------------------- |\n",
    "| 금융 분야 주요 활용 사례   | 신용 불이행 예측, 사기 탐지, 대출 승인 결정, 매수/매도 추천             |\n",
    "| 특성 정규화/표준화 필요 여부 | 권장됨                                              |\n",
    "| Python 라이브러리 설치  | ```pip install numpy pandas matplotlib scikit-learn``` |\n",
    "\n",
    "로지스틱 회귀는 금융에서 이진 분류 문제(예: 불이행/정상, 매수/매도)에 널리 사용됩니다. 이 기법은 확률을 제공하며 다중 클래스 분류로 확장될 수 있습니다. 그러나 예측 변수와 결과의 로그 오즈(log odds) 사이에 선형 관계가 있다는 가정을 하므로 모든 경우에 적합하지 않을 수 있습니다.\n",
    "\n",
    "금융 분야에서는 로지스틱 회귀를 통해 대출 신청자의 불이행 여부, 거래의 사기 여부, 고객의 금융 상품 구매 여부 등을 예측할 수 있습니다. 다양한 결과의 가능성을 추정함으로써 금융 기관이 정보에 기반한 결정을 내리고, 리스크를 관리하며, 타겟 마케팅 전략을 개발할 수 있도록 돕습니다.\n",
    "\n",
    "예측 변수의 스케일이 다를 경우, 모델 수렴을 원활하게 하고 해석력을 높이기 위해 정규화 또는 표준화를 권장합니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)\n",
    "true_coeffs = np.array([2.5, -1.5, 0, 0, 3, 0, 0, 0, 1, 0])\n",
    "logits = X @ true_coeffs + np.random.randn(100) * 2\n",
    "y = (logits > np.median(logits)).astype(int)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 로지스틱 회귀 모델 학습\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ROC 곡선 그리기\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 평가 지표 출력\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test, model.predict(X_test_scaled)))\n",
    "```\n",
    "\n",
    "**분류 리포트**\n",
    "\n",
    "|    클래스   |  정밀도 |  재현율 | F1 점수 | 지원 수 |\n",
    "| :------: | :--: | :--: | :---: | :--: |\n",
    "|     0    | 0.55 | 0.75 |  0.63 |   8  |\n",
    "|     1    | 0.78 | 0.58 |  0.67 |  12  |\n",
    "|    정확도   |      |      |  0.65 |  20  |\n",
    "|  매크로 평균  | 0.66 | 0.67 |  0.65 |  20  |\n",
    "| 가중 평균 점수 | 0.68 | 0.65 |  0.65 |  20  |\n",
    "\n",
    "<img src=\"./images/fig_05_15.png\" width=800>\n",
    "\n",
    "\n",
    "혼동 행렬\n",
    "```\n",
    "[[6, 2]\n",
    "[5, 7]]\n",
    "```\n",
    "혼동 행렬 의미\n",
    "\n",
    "|       | 양성 예측 | 음성 예측 |\n",
    "| :---- | :---: | :---: |\n",
    "| 실제 양성 |   TP  |   FN  |\n",
    "| 실제 음성 |   FP  |   TN  |\n",
    "\n",
    "**핵심 지표**\n",
    "\n",
    "1. 정확도:\n",
    "\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "2. 정밀도:\n",
    "\n",
    "$$\n",
    "\\text{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "3. 재현율:\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "4. F1 점수:\n",
    "\n",
    "$$\n",
    "\\text{F1 Score} = \\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "$$\n",
    "\n",
    "**히든 마르코프 모델**\n",
    "\n",
    "|        설명        | 히든 마르코프 모델(HMM)은 숨겨진 상태를 가진 시계열 데이터를 모델링하는 강력한 프레임워크로, 시장 체제 전환을 포착할 수 있습니다. |\n",
    "| :--------------: | :--------------------------------------------------------------------------: |\n",
    "|  금융 분야 주요 활용 사례  |                  시장 체제 모델링, 시계열 패턴 인식, 경기 순환 예측, 트레이딩 전략 향상                  |\n",
    "| 특성 정규화/표준화 필요 여부 |                                 필수는 아니지만 권장됨                                 |\n",
    "|  Python 라이브러리 설치 |                 ```pip install numpy pandas matplotlib hmmlearn```                 |\n",
    "\n",
    "히든 마르코프 모델(HMM)은 숨겨진 상태를 가진 시스템을 표현하는 통계 모델로, 상태 간 전이 확률을 기반으로 상태를 예측합니다. 하지만 학습이 복잡하고 파라미터 추정이 까다로울 수 있습니다.\n",
    "\n",
    "금융 분야에서는 HMM을 통해 시장 체제를 모델링하거나 시계열 데이터에서 패턴을 감지하며, 상태 간 확률 전이를 통해 예측 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "여러 특성이 있을 경우, 정규화 또는 표준화는 수렴 속도와 해석력을 높이는 데 도움이 됩니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 200\n",
    "X = np.linspace(0, 20, n)\n",
    "state_1 = 2.5 * np.sin(X[:n//2]) + np.random.randn(n//2) * 0.5\n",
    "state_2 = -1.5 * np.sin(X[n//2:]) + 2 + np.random.randn(n//2) * 0.5\n",
    "y = np.concatenate([state_1, state_2])\n",
    "\n",
    "# 히든 마르코프 모델 학습\n",
    "model = hmm.GaussianHMM(n_components=2, covariance_type=\"full\", n_iter=100)\n",
    "model.fit(y.reshape(-1, 1))\n",
    "\n",
    "# 숨겨진 상태 예측\n",
    "hidden_states = model.predict(y.reshape(-1, 1))\n",
    "\n",
    "# 숨겨진 상태와 함께 시계열 그래프 출력\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, y, label='Observed', color='blue')\n",
    "plt.plot(X[hidden_states == 0], y[hidden_states == 0], 'ro', label='State 1')\n",
    "plt.plot(X[hidden_states == 1], y[hidden_states == 1], 'go', label='State 2')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('Hidden Markov Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 적합 통계 출력\n",
    "print('Transition Matrix')\n",
    "print(model.transmat_)\n",
    "print('Means and Variances of Each State')\n",
    "print(model.means_)\n",
    "print(model.covars_)\n",
    "```\n",
    "\n",
    "전이 행렬\n",
    "```\n",
    "[[0.98722454 0.01277546]\n",
    "[0.05087947 0.94912053]]\n",
    "```\n",
    "\n",
    "각 상태의 평균 및 분산\n",
    "```\n",
    "[[ 2.05014575]\n",
    "[-1.43768284]]\n",
    "\n",
    "[[[1.13209908]]\n",
    "[[1.05456691]]]\n",
    "```\n",
    "<img src=\"./images/fig_05_16.png\" width=800>\n",
    "\n",
    "\n",
    "**가우시안 나이브 베이즈**\n",
    "\n",
    "| 설명               | 가우시안 나이브 베이즈는 확률 기반의 분류 방법으로, 특히 고차원 데이터셋에서 유용하게 작동합니다. |\n",
    "| :--------------- | :------------------------------------------------------ |\n",
    "| 금융 분야 주요 활용 사례   | 신용 리스크 분류, 시장 동향 예측, 거래 데이터의 이상 탐지, 사기 탐지               |\n",
    "| 특성 정규화/표준화 필요 여부 | 필수는 아니지만 권장됨                                            |\n",
    "| Python 라이브러리 설치  | ```pip install numpy pandas matplotlib scikit-learn```        |\n",
    "\n",
    "가우시안 나이브 베이즈 알고리즘은 스팸 탐지, 질병 진단, 사기 탐지 등에 사용되는 분류 기법입니다. 이 기법은 베이즈 정리를 기반으로 하며, 각 특성이 정규분포(가우시안)를 따른다고 가정합니다. 학습 데이터로부터 각 특성의 평균과 분산을 학습한 후, 이를 바탕으로 각 클래스에 대한 확률을 추정하고, 가장 높은 확률을 가지는 클래스를 새로운 데이터에 대해 선택합니다.\n",
    "\n",
    "가우시안 나이브 베이즈는 정규분포를 따르는 특성에 대해 효과적인 분류 성능을 보입니다. 구현과 해석이 간단하지만, 특성 분포가 정규분포에서 크게 벗어나거나 특성 간 독립성 가정이 성립하지 않을 경우 성능이 저하될 수 있습니다.\n",
    "\n",
    "이 기법은 금융 분야에서 리스크 분류, 시장 움직임 예측, 거래 패턴의 이상 탐지에 활용됩니다.\n",
    "\n",
    "특성 간 스케일이 다르거나 정규분포를 따르지 않는 경우, 정규화 또는 표준화는 모델 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 10)  # 특성: 10개의 무작위 값\n",
    "true_coeffs = np.array([2.5, -1.5, 0, 0, 3, 0, 0, 0, 1, 0])\n",
    "logits = X @ true_coeffs + np.random.randn(100) * 2\n",
    "y = (logits > np.median(logits)).astype(int)  # 타깃: 로짓 기반 이진 결과\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 가우시안 나이브 베이즈 모델 사용\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 이진 분류의 경우 ROC 곡선 그리기\n",
    "y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 평가 지표 출력\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, model.predict(X_test_scaled)))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, model.predict(X_test_scaled)))\n",
    "```\n",
    "\n",
    "| Classification Report: |        |          |         |    |\n",
    "| ---------------------: | -----: | -------: | ------: | -: |\n",
    "|              precision | recall | f1-score | support |    |\n",
    "|                      0 |   0.64 |     0.88 |    0.74 |  8 |\n",
    "|                      1 |   0.89 |     0.67 |    0.76 | 12 |\n",
    "|               accuracy |        |          |    0.75 | 20 |\n",
    "|              macro avg |   0.76 |     0.77 |    0.75 | 20 |\n",
    "|           weighted avg |   0.79 |     0.75 |    0.75 | 20 |\n",
    "\n",
    "혼동 행렬:\n",
    "\n",
    "```\n",
    "[[7 1]\n",
    " [4 8]]\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_17.png\" width=800>\n",
    "\n",
    "**합성곱 신경망**\n",
    "\n",
    "| 설명              | 합성곱 신경망(CNN)은 원래 이미지 처리에 사용되었으나, 금융 시계열 데이터에 적용되어 복잡한 패턴과 추세를 포착할 수 있습니다. |\n",
    "| :-------------- | :------------------------------------------------------------------------ |\n",
    "| 금융 분야 주요 활용 사례  | 주가 예측, 금융 데이터 이상 탐지, 고빈도 매매, 금융 뉴스에서의 감성 분석                               |\n",
    "| 특성              | 필수                                                                        |\n",
    "| 정규화/표준화         | 필수                                                                        |\n",
    "| Python 라이브러리 설치 | ```pip install numpy pandas matplotlib scikit-learn tensorflow```               |\n",
    "\n",
    "인간 두뇌의 작동 방식을 모방하려는 머신러닝 모델을 신경망이라 부릅니다. 이러한 모델은 \"뉴런\"이라고 불리는 노드가 층별로 상호 연결된 구조를 가지고 있으며, 수학적 함수를 이용해 입력 데이터를 출력으로 변환합니다. 학습은 정답을 예측하기 위해 이 연결(또는 가중치)을 조정하며 이루어집니다.\n",
    "\n",
    "CNN은 원래 이미지 처리를 위해 고안된 딥러닝 모델이지만, 금융 분야에서는 시계열 예측을 포함한 다양한 응용 분야로 확장되었습니다. CNN은 단순한 모델로는 포착하기 어려운 복잡한 패턴과 종속성을 학습할 수 있습니다. CNN은 높은 예측 성능과 유연성을 제공하지만, 계산 자원이 많이 필요하고 세심한 튜닝이 요구되며, 데이터셋이 작거나 모델의 해석력이 중요한 경우에는 부적합할 수 있습니다.\n",
    "\n",
    "금융 분야에서 CNN은 주가 예측, 이상 탐지, 금융 시계열 데이터 분석에 활용됩니다.\n",
    "\n",
    "정규화 또는 표준화는 학습 수렴 속도를 높이고 성능을 향상시키기 위해 필수적입니다.\n",
    "\n",
    "효과적인 신경망 모델을 구축하기 위한 첫 단계는 적절한 모델 아키텍처를 선택하고 층의 수를 결정하는 것입니다. 주요 모델 아키텍처로는 단순 패턴 인식에 적합한 피드포워드 신경망, 복잡한 패턴 인식에 적합한 CNN, 시계열 및 문맥 이해에 적합한 순환 신경망(RNN), 자연어 처리에 적합한 트랜스포머 모델이 있습니다.\n",
    "\n",
    "아키텍처를 선택한 후, 모델의 구조를 설계합니다. 이 단계에서는 입력층 크기, 은닉층 수 및 연결 방식, 출력층 크기를 정의해야 합니다.\n",
    "\n",
    "모델의 성능을 최적화하려면 하이퍼파라미터 선택과 과적합 방지 기법이 중요합니다. 주요 하이퍼파라미터로는 모델 성능을 측정하는 손실 함수와, 손실 값을 기반으로 가중치를 업데이트하는 옵티마이저가 있습니다. 과적합 방지를 위한 기법으로는 정규화, 드롭아웃, 조기 종료, 데이터 증강 등이 있으며, 모델이 새로운 데이터에 잘 일반화되도록 돕습니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Dropout, LSTM\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(0)\n",
    "n = 1000\n",
    "X = np.linspace(0, 1000, n)\n",
    "y = np.sin(X / 100)\n",
    "\n",
    "# 사인파 그리기\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X, y, label='sin(x)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('sin(X)')\n",
    "plt.title('Sine Wave')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# CNN 입력을 위한 데이터 재구성\n",
    "X = X.reshape(-1, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# 학습 데이터와 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# 특성 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# CNN 입력 형태로 리셰이프\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], 1, 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], 1, 1))\n",
    "\n",
    "# 회귀를 위한 CNN-LSTM 모델 구성\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu', input_shape=(1, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(50, return_sequences=True))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# 모델 학습 (로그 출력 포함)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=300, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "# 테스트 데이터에 대한 예측 및 시각화\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test, y_test, color='green', label='Test data')\n",
    "plt.scatter(X_test, y_pred, color='red', label='Fitted model')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Target')\n",
    "plt.title('CNN-LSTM Neural Network Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 모델 성능 지표 출력\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R^2 Score: {r2}')\n",
    "\n",
    "# 학습 및 검증 손실 값 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('Model loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "| 층                  | 입력 형태         | 출력 형태         |\n",
    "| :----------------- | :------------ | :------------ |\n",
    "| conv1d (Conv1D)    | (None, 1, 1)  | (None, 1, 64) |\n",
    "| conv1d\\_1 (Conv1D) | (None, 1, 64) | (None, 1, 64) |\n",
    "| conv1d\\_2 (Conv1D) | (None, 1, 64) | (None, 1, 64) |\n",
    "| dropout (Dropout)  | (None, 1, 64) | (None, 1, 64) |\n",
    "| lstm (LSTM)        | (None, 1, 64) | (None, 1, 50) |\n",
    "| lstm\\_1 (LSTM)     | (None, 1, 50) | (None, 50)    |\n",
    "| dense (Dense)      | (None, 50)    | (None, 1)     |\n",
    "\n",
    "```\n",
    "Epoch 1/300\n",
    "20/20 - 2s 13ms/step - loss: 0.4776 - val\\_loss: 0.3855\n",
    "Epoch 300/300\n",
    "20/20 - 0s 3ms/step - loss: 0.0018 - val\\_loss: 0.0759\n",
    "7/7 - 0s 27ms/step\n",
    "Mean Squared Error: 0.08142697718792646\n",
    "R^2 Score: 0.8100884046552388\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_05_18.png\" width=800>\n",
    "\n",
    "<img src=\"./images/fig_05_19.png\" width=800>\n",
    "\n",
    "모델 손실 그래프는 다음을 나타냅니다:\n",
    "\n",
    "* 학습 손실은 지속적으로 감소하여 모델이 학습 데이터에서 잘 학습하고 있음을 보여줍니다.\n",
    "* 그러나 검증 손실은 증가하고 있으며, 이는 모델이 과적합되고 있음을 의미합니다. 즉, 모델이 학습 데이터의 노이즈와 세부사항을 학습하고 있어 새로운 데이터에 일반화되지 못하고 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 순위 매김\n",
    "\n",
    "**LGBRanker 순위화**\n",
    "\n",
    "|        설명        |                                LGBRanker는 순위화 목적 최적화를 위해 설계된 LightGBM 라이브러리의 순위화 알고리즘입니다.                                |\n",
    "| :--------------: | :----------------------------------------------------------------------------------------------------------------------: |\n",
    "|  금융 분야 주요 활용 사례  | 신용 리스크 모델링(채무 불이행 위험 기준 고객 순위화), 추천 시스템(고객 맞춤 금융 상품 우선순위화), 고객 가치 평가(잠재 가치 또는 리스크 기준 고객 순위화), 포트폴리오 최적화(투자 결정 기준 자산 순위화) |\n",
    "| 특성 정규화/표준화 필요 여부 |                                                       필수는 아니지만 권장됨                                                       |\n",
    "|  Python 라이브러리 설치 |                                    ```pip install numpy lightgbm matplotlib scikit-learn```                                    |\n",
    "\n",
    "LGBRanker는 대규모 복잡한 데이터셋에서 정확한 정렬 결과를 생성하기 위해 순위화 전용 목적 함수를 최적화하는 LightGBM 라이브러리의 알고리즘입니다. LGBRanker는 LambdaRank를 기반 최적화 기법으로 활용하는 순위화 알고리즘입니다.\n",
    "\n",
    ">\"Learning-to-rank는 정보 검색 시스템에서 순위 모델을 구축하기 위한 머신러닝의 응용입니다. 학습 데이터는 각 항목 리스트에 대해 일부 순서가 지정된 항목 목록으로 구성되며, 순위 모델의 목적은 새로운, 보지 못한 리스트에 대해 항목의 순서를 산출하는 것입니다.\" — Xindi Wang\n",
    "\n",
    "금융 분야의 주요 응용 사례로는 대출 불이행 가능성에 따라 대출 신청자를 순위화하는 신용 리스크 모델링, 거래 이력 및 행동 데이터를 기반으로 금융 상품을 추천하는 추천 시스템, 금융 기관에 대한 잠재 가치 기준 고객 순위화, 예상 수익률 및 리스크 프로파일을 기준으로 자산을 순위화하는 포트폴리오 최적화 등이 있습니다.\n",
    "\n",
    "트리 기반 알고리즘인 LGBRanker는 정규화 또는 표준화를 필수로 요구하지 않지만, 특성 간 스케일 차이가 클 경우 성능 향상에 도움이 됩니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRanker\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# 1단계: 주식 데이터 샘플 생성\n",
    "np.random.seed(42)\n",
    "n_stocks = 100\n",
    "n_groups = 10\n",
    "historical_returns = np.random.rand(n_stocks, 1)\n",
    "volatility = np.random.rand(n_stocks, 1)\n",
    "momentum = np.random.rand(n_stocks, 1)\n",
    "X = np.hstack((historical_returns, volatility, momentum))\n",
    "true_rank = historical_returns * 0.5 + volatility * 0.3 + momentum * 0.2\n",
    "y = np.argsort(np.argsort(true_rank.flatten()))\n",
    "y = np.digitize(y, bins=np.linspace(0, n_stocks, 32)) - 1\n",
    "groups = np.repeat(np.arange(n_groups), n_stocks // n_groups)\n",
    "\n",
    "# 2단계: 학습 및 테스트 데이터 분할\n",
    "X_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n",
    "    X, y, groups, test_size=0.2, random_state=42, stratify=groups)\n",
    "train_group_sizes = np.bincount(group_train)\n",
    "test_group_sizes = np.bincount(group_test)\n",
    "train_group_sizes = train_group_sizes[train_group_sizes != 0]\n",
    "test_group_sizes = test_group_sizes[test_group_sizes != 0]\n",
    "\n",
    "# 3단계: LGBMRanker 초기화 및 학습\n",
    "model = LGBMRanker(\n",
    "    objective=\"lambdarank\",\n",
    "    metric=\"ndcg\",\n",
    "    boosting_type=\"gbdt\",\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    ndcg_eval_at=[1, 3, 5]\n",
    ")\n",
    "model.fit(X_train, y_train, group=train_group_sizes, eval_set=[(X_test, y_test)], eval_group=[test_group_sizes])\n",
    "\n",
    "# 4단계: 예측 및 평가\n",
    "y_pred = model.predict(X_test)\n",
    "unique_groups = np.unique(group_test)\n",
    "valid_groups = [g for g in unique_groups if np.sum(group_test == g) > 1]\n",
    "ndcg_scores = [ndcg_score([y_test[group_test == g]], [y_pred[group_test == g]], k=5) for g in valid_groups]\n",
    "mean_ndcg = np.mean(ndcg_scores)\n",
    "print(f'Mean NDCG Score @5: {mean_ndcg}')\n",
    "\n",
    "# 결과 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "for group in valid_groups:\n",
    "    plt.scatter(y_test[group_test == group], y_pred[group_test == group], alpha=0.6, label=f'Group {group}')\n",
    "plt.xlabel('True Rankings')\n",
    "plt.ylabel('Predicted Rankings')\n",
    "plt.title('True vs. Predicted Rankings for Multiple Groups')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "```\n",
    "Mean NDCG Score @5: 0.9859718699852197\n",
    "```\n",
    "\n",
    "정규화 할인 누적 이득(NDCG)은 순위 품질을 평가하기 위한 기준 성능 지표입니다. NDCG 점수는 0에서 1 사이의 값을 가지며, 1은 완벽한 순위를 의미합니다. \"@5\"는 정렬된 목록의 상위 5개 항목을 기준으로 평가했음을 의미합니다.\n",
    "\n",
    "그림 5.20은 NDCG Score @5와 유사한 해석을 전달합니다 — 대부분의 점이 대각선 근처에 위치해 있으며, 이는 모델이 실제 순위와 일치하는 순위를 잘 예측하고 있음을 나타냅니다.\n",
    "\n",
    "<img src=\"./images/fig_05_20.png\" width=800>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 클러스터링\n",
    "\n",
    "**OPTICS 클러스터링**\n",
    "\n",
    "| 설명               | 밀도 기반 클러스터링 알고리즘으로, 클러스터 개수를 알 수 없고 노이즈가 포함된 데이터에서 다양한 밀도의 클러스터를 식별합니다. |\n",
    "| :--------------- | :---------------------------------------------------------------------- |\n",
    "| 금융 분야 주요 활용 사례   | 거래 데이터의 이상 탐지, 거래 패턴 기반 고객 세분화, 시장 체제 식별                                |\n",
    "| 특성 정규화/표준화 필요 여부 | 권장됨                                                                     |\n",
    "| Python 라이브러리 설치  | ```pip install numpy matplotlib scikit-learn```                               |\n",
    "\n",
    "클러스터링은 복잡한 데이터셋에서 인사이트와 패턴을 식별하기 위한 기법입니다. OPTICS(Ordering Points To Identify the Clustering Structure)는 클러스터 개수를 사전에 알 수 없는 노이즈 포함 데이터에 자주 사용되는 알고리즘 중 하나입니다. k-평균과 같은 기존 클러스터링 기법이 사전 클러스터 개수 설정을 필요로 하는 반면, OPTICS는 밀도 도달 가능성과 중심 거리(core distance)에 따라 데이터 포인트를 처리함으로써, 사전 정의 없이 클러스터 구조를 발견할 수 있도록 해줍니다.\n",
    "\n",
    "금융 분야에서의 주요 응용 사례는 거래 데이터의 이상 탐지, 고객 행동 패턴 기반 세분화, 시장 체제 식별 등이 있습니다.\n",
    "\n",
    "최적의 성능을 위해 모든 특성이 클러스터링 과정에 균등하게 기여하도록 특성 정규화를 권장합니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "다음은 sklearn.datasets의 make\\_blobs 메서드를 사용하여 OPTICS 알고리즘을 적용한 예제입니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "X, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.6, random_state=42)\n",
    "\n",
    "# 특성 정규화 (feature normalization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# OPTICS 클러스터링 적용\n",
    "optics = OPTICS(min_samples=10, xi=0.05, min_cluster_size=0.1)\n",
    "optics.fit(X_scaled)\n",
    "\n",
    "# 라벨 및 핵심 샘플 인덱스 추출\n",
    "labels = optics.labels_\n",
    "core_samples = np.zeros_like(labels, dtype=bool)\n",
    "\n",
    "# 수치 성능 지표 출력\n",
    "# -1은 노이즈이므로 제외하고 평가\n",
    "mask = labels != -1\n",
    "print(\"Silhouette Score:\", silhouette_score(X_scaled[mask], labels[mask]))\n",
    "print(\"Davies-Bouldin Index:\", davies_bouldin_score(X_scaled[mask], labels[mask]))\n",
    "\n",
    "# 클러스터링 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        col = [0, 0, 0, 1]\n",
    "    class_member_mask = (labels == k)\n",
    "    xy = X_scaled[class_member_mask & core_samples]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=14)\n",
    "    xy = X_scaled[class_member_mask & ~core_samples]\n",
    "    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n",
    "\n",
    "plt.title('OPTICS Clustering')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과:\n",
    "```\n",
    "Silhouette Score: 0.9007128264042205\n",
    "Davies-Bouldin Index: 0.13502085379361267\n",
    "```\n",
    "\n",
    "실루엣 점수는 각 포인트가 자신의 클러스터와 얼마나 유사한지를 측정하며 값이 높을수록 좋습니다. 데이비스-볼딘 지수는 각 클러스터가 얼마나 다른지를 측정하며 값이 낮을수록 좋습니다.\n",
    "\n",
    "그림 5.21에서 검은색은 노이즈를 나타냅니다.\n",
    "\n",
    "<img src=\"./images/fig_05_21.png\" width=800>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델\n",
    "\n",
    "**OpenAI 언어 모델**\n",
    "\n",
    "|        설명        | 대형 언어 모델(LLMs)은 인간과 유사한 텍스트를 이해하고 생성할 수 있는 모델로, 대량의 금융 데이터를 처리하고 분석하는 데 유용합니다. |\n",
    "| :--------------: | :----------------------------------------------------------------------------: |\n",
    "|  금융 분야 주요 활용 사례  |                             시장 분석 및 리서치, 규제 준수, 리포팅                            |\n",
    "| 특성 정규화/표준화 필요 여부 |                                     필요하지 않음                                    |\n",
    "|  Python 라이브러리 설치 |                             ```pip install openai json```                            |\n",
    "\n",
    "OpenAI의 GPT 시리즈와 같은 대형 언어 모델(LLMs)은 방대한 텍스트 데이터셋을 기반으로 개발됩니다. 이들은 문서 요약, 언어 번역, 질의 응답, 텍스트 작성, 감정 분석 등의 작업을 수행할 수 있도록 설계되어 있으며, 인간의 문장을 닮은 텍스트를 생성할 수 있습니다.\n",
    "\n",
    "LLMs는 방대한 데이터를 기반으로 언어 패턴을 분석하고 해석하며, 일관성 있고 문맥적으로 적절한 텍스트를 생성할 수 있는 고급 AI 시스템입니다. 비정형 데이터에서 인사이트를 도출할 수 있다는 점이 강점입니다. 하지만 계산 자원이 많이 필요하며, 높은 성능을 위해서는 많은 데이터가 요구됩니다.\n",
    "\n",
    "금융에서는 감정 분석, 자동 리포트 생성, 예측 분석, 리스크 관리 등에 사용됩니다.\n",
    "\n",
    "텍스트 기반의 데이터에서는 일반적으로 정규화가 필요하지 않지만, 숫자형 데이터와 함께 사용할 경우 모델 성능과 일관성을 향상시키기 위해 정규화를 고려할 수 있습니다.\n",
    "\n",
    "**Python 예제**\n",
    "\n",
    "```python\n",
    "import openai\n",
    "import json\n",
    "\n",
    "# OpenAI API 키 설정\n",
    "openai.api_key = 'YOUR_API_KEY_HERE'\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Analyze the sentiment of the following text and provide a sentiment score from 0 to 10 in JSON in the property called sentiment_score:\\n\\n{text}\\n\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return json.loads(response.choices[0].message.content)['sentiment_score']\n",
    "\n",
    "def test_analyze_sentiment():\n",
    "    test_sentences = [\n",
    "        \"I absolutely love this new phone! It's fantastic and exceeds all my expectations\",\n",
    "        \"This is the worst experience I have ever had with a company. Totally unacceptable\",\n",
    "        \"Meh, the movie was just okay. Not too good, not too bad\",\n",
    "        \"I am thrilled with my new job. The team is great and the work is fulfilling\",\n",
    "        \"The food was terrible, and the service was even worse. I am never coming back here\",\n",
    "        \"Wow, what a beautiful day! The sun is shining and everything feels perfect\",\n",
    "        \"This product is a complete waste of money. It broke after one use\",\n",
    "        \"I am feeling pretty neutral about this situation, neither happy nor sad\",\n",
    "        \"The concert last night was amazing! Best performance I've ever seen\",\n",
    "        \"Ugh, what a horrible traffic jam. Made me late for work and ruined my day\"\n",
    "    ]\n",
    "\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        try:\n",
    "            sentiment_score = analyze_sentiment(sentence)\n",
    "            print(f\"Test {i+1}: Sentence: \\\"{sentence}\\\"\")\n",
    "            print(f\"Sentiment Score: {sentiment_score}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Test {i+1}: Sentence: \\\"{sentence}\\\"\")\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# 테스트 함수 실행\n",
    "test_analyze_sentiment()\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
