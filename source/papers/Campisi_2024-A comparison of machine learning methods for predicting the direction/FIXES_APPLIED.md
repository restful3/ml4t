# Code Fixes Applied - Campisi et al. (2024) Replication

**ìƒì„±ì¼**: 2025-12-20
**ì›ë³¸ íŒŒì¼**: `campisi_2024_replication.py`
**ìˆ˜ì • íŒŒì¼**: `campisi_2024_replication_fixed.py`

## Executive Summary

Geminiì˜ ì½”ë“œ ë¦¬ë·° ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ **Data Leakage** ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ , í†µê³„ì  ê²€ì • ë° ì„±ëŠ¥ ìµœì í™”ë¥¼ ì¶”ê°€í•œ ê°œì„  ë²„ì „ì„ ì‘ì„±í–ˆìŠµë‹ˆë‹¤.

---

## ì£¼ìš” ìˆ˜ì • ì‚¬í•­

### ğŸš¨ Critical Issues (ë°˜ë“œì‹œ ìˆ˜ì •)

#### 1. Data Leakage: Standardization ë¬¸ì œ í•´ê²°

**ì›ë³¸ ì½”ë“œ (WRONG)**:
```python
# Line 982: CV loop ë°–ì—ì„œ ì „ì²´ ë°ì´í„° í‘œì¤€í™”
X_scaled, scaler = standardize_features(X)  # âŒ ë¯¸ë˜ ë°ì´í„° ìœ ì¶œ

# Inside CV loop
for train_idx, test_idx in cv.split(X_scaled):
    model.fit(X_train, y_train)  # ì´ë¯¸ ë¯¸ë˜ ì •ë³´ë¡œ ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ì‚¬ìš©
```

**ìˆ˜ì • ì½”ë“œ (CORRECT)**:
```python
# Pipeline ì‚¬ìš©ìœ¼ë¡œ CV loop ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬
from sklearn.pipeline import Pipeline

def get_classification_models(use_feature_selection=True):
    models = {}
    for name, base_model in base_models.items():
        steps = [
            ('scaler', StandardScaler()),  # âœ… CV ë‚´ë¶€ì—ì„œ fit
            ('classifier', base_model)
        ]
        models[name] = Pipeline(steps)
    return models

# Train í•¨ìˆ˜ì—ì„œ
for train_idx, test_idx in cv.split(X):  # XëŠ” raw data
    X_train, X_test = X[train_idx], X[test_idx]
    pipeline.fit(X_train, y_train)  # âœ… trainì—ì„œë§Œ scaler fit
    pred = pipeline.predict(X_test)  # âœ… testëŠ” transformë§Œ
```

**íš¨ê³¼**:
- Train setì˜ í‰ê· /ë¶„ì‚°ë§Œ ì‚¬ìš©í•˜ì—¬ ìŠ¤ì¼€ì¼ë§
- Test set ì •ë³´ ìœ ì¶œ ë°©ì§€
- ì •í™•í•œ out-of-sample ì„±ëŠ¥ ì¸¡ì •

---

#### 2. Data Leakage: Feature Selection ë¬¸ì œ í•´ê²°

**ì›ë³¸ ì½”ë“œ (WRONG)**:
```python
# Line 990: CV loop ë°–ì—ì„œ ì „ì²´ ë°ì´í„°ë¡œ feature selection
selected_features, importance = lasso_feature_selection(
    X_scaled, y_continuous, feature_cols
)  # âŒ 2012ë…„ ëª¨ë¸ì´ 2021ë…„ ì •ë³´ë¥¼ ì•Œê³  ìˆìŒ

X_selected = X_scaled[:, selected_idx]
```

**ìˆ˜ì • ì½”ë“œ (CORRECT)**:
```python
# Pipelineì— SelectFromModel ì¶”ê°€
from sklearn.feature_selection import SelectFromModel

steps = [
    ('scaler', StandardScaler()),
    ('selector', SelectFromModel(  # âœ… CV ë‚´ë¶€ì—ì„œ ìë™ feature selection
        Lasso(alpha=lasso_alpha, random_state=RANDOM_STATE),
        threshold=1e-5
    )),
    ('classifier', base_model)
]
pipeline = Pipeline(steps)

# CV loopì—ì„œ ìë™ìœ¼ë¡œ train dataë¡œë§Œ feature selection ìˆ˜í–‰
pipeline.fit(X_train, y_train)
```

**íš¨ê³¼**:
- ê° CV iterationë§ˆë‹¤ í•´ë‹¹ ì‹œì ì˜ train dataë¡œë§Œ feature ì„ íƒ
- ë¯¸ë˜ ì •ë³´ í™œìš© ë°©ì§€
- ì‹œê°„ì— ë”°ë¥¸ feature ì¤‘ìš”ë„ ë³€í™” ë°˜ì˜

---

#### 3. Diebold-Mariano Test êµ¬í˜„ ë° í˜¸ì¶œ

**ì›ë³¸ ì½”ë“œ**:
```python
# Line 562: í•¨ìˆ˜ë§Œ ì •ì˜ë˜ê³  í˜¸ì¶œ ì•ˆ ë¨
def diebold_mariano_test(y_true, pred1, pred2):
    # ... êµ¬í˜„ì€ ë˜ì–´ ìˆìŒ
    pass

# main()ì—ì„œ í˜¸ì¶œë˜ì§€ ì•ŠìŒ âŒ
```

**ìˆ˜ì • ì½”ë“œ**:
```python
def perform_dm_tests(results: Dict, y_true: np.ndarray) -> pd.DataFrame:
    """Perform pairwise Diebold-Mariano tests between models."""
    model_names = list(results.keys())
    dm_results = []

    for i, name1 in enumerate(model_names):
        for j, name2 in enumerate(model_names):
            if i < j:
                pred1 = results[name1]['predictions']
                pred2 = results[name2]['predictions']
                dm_stat, p_val = diebold_mariano_test(y_true, pred1, pred2)
                dm_results.append({
                    'Model 1': name1,
                    'Model 2': name2,
                    'DM Statistic': dm_stat,
                    'p-value': p_val,
                    'Significant (5%)': p_val < 0.05
                })

    return pd.DataFrame(dm_results)

# main()ì—ì„œ í˜¸ì¶œ âœ…
dm_tests_clf = perform_dm_tests(clf_results_after, y_true_binary)
dm_tests_reg = perform_dm_tests(reg_results_after, y_true_reg_binary)
```

**íš¨ê³¼**:
- ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œì§€ ê²€ì •
- ë…¼ë¬¸ì˜ í•µì‹¬("A comparison of ML methods") ì¦ëª… ê°€ëŠ¥

---

### âš¡ Performance & Optimization

#### 1. Refit Frequency ì˜µì…˜ ì¶”ê°€

**ë¬¸ì œ**: ë§¤ì¼ ëª¨ë“  ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ë©´ ê³„ì‚°ëŸ‰ì´ ê³¼ë„í•¨
- 11 models Ã— ~750 iterations = ~8,250 training runs

**ìˆ˜ì •**:
```python
class WalkForwardCV:
    def __init__(self, train_size=TRAIN_SIZE, gap=GAP,
                 max_iterations=None,
                 refit_frequency=1):  # âœ… ìƒˆë¡œ ì¶”ê°€
        self.refit_frequency = refit_frequency

    def split(self, X):
        for iteration_count, test_idx in enumerate(...):
            should_refit = (
                last_train_idx is None or
                iteration_count % self.refit_frequency == 0
            )
            yield train_idx, test_idx, should_refit

# ì‚¬ìš© ì˜ˆ
cv = WalkForwardCV(refit_frequency=30)  # 30ì¼ë§ˆë‹¤ ì¬í•™ìŠµ
```

**íš¨ê³¼**:
- ê³„ì‚° ì‹œê°„ ëŒ€í­ ê°ì†Œ (refit_frequency=30 ì‹œ ~96% ê°ì†Œ)
- ì‹¤ë¬´ì ìœ¼ë¡œ í•©ë¦¬ì  (ë§¤ì¼ ì¬í•™ìŠµì€ í˜„ì‹¤ì ì´ì§€ ì•ŠìŒ)

---

#### 2. Command Line Arguments ì¶”ê°€

```python
parser.add_argument('--refit-frequency', '-r', type=int, default=1,
                    help='ëª¨ë¸ ì¬í•™ìŠµ ì£¼ê¸° (ì¼ ë‹¨ìœ„)')
parser.add_argument('--no-feature-selection', action='store_true',
                    help='Feature selection ë¹„í™œì„±í™”')
```

**ì‚¬ìš© ì˜ˆ**:
```bash
# ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (10 iterations, 30ì¼ë§ˆë‹¤ ì¬í•™ìŠµ)
python campisi_2024_replication_fixed.py -m 10 -r 30

# Feature selection ì—†ì´ ì‹¤í–‰
python campisi_2024_replication_fixed.py --no-feature-selection

# ì „ì²´ ì‹¤í–‰ (ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
python campisi_2024_replication_fixed.py
```

---

## ì½”ë“œ êµ¬ì¡° ê°œì„ 

### Before (Original)
```
Phase 1-2: Data Collection
Phase 3: Preprocessing
Phase 4: Feature Selection (âŒ ì—¬ê¸°ì„œ leakage ë°œìƒ)
  - standardize_features(X)  # ì „ì²´ ë°ì´í„°
  - lasso_feature_selection(X_scaled, y)  # ì „ì²´ ë°ì´í„°
Phase 5-7: Model Training
  - CV loopì—ì„œ ì´ë¯¸ ìŠ¤ì¼€ì¼ë§/ì„ íƒëœ ë°ì´í„° ì‚¬ìš©
```

### After (Fixed)
```
Phase 1-2: Data Collection
Phase 3: Preprocessing
Phase 4: Prepare Data (raw dataë§Œ ì¤€ë¹„)
  - X = data[feature_cols].values  # âœ… raw data
Phase 5-7: Model Training with Pipeline
  - Pipelineì´ CV loop ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ì²˜ë¦¬
    1. StandardScaler().fit(X_train)
    2. SelectFromModel().fit(X_train, y_train)
    3. Classifier.fit(X_train_transformed, y_train)
Phase 8: Diebold-Mariano Tests (âœ… ìƒˆë¡œ ì¶”ê°€)
Phase 9-10: Visualization & Report
```

---

## ì˜ˆìƒ ì„±ëŠ¥ ë³€í™”

### ì›ë³¸ ì½”ë“œ (Data Leakage ìˆìŒ)
- ë†’ì€ Accuracy (ì˜ˆ: ~0.82)
- ê³¼ë„í•˜ê²Œ ë‚™ê´€ì ì¸ ê²°ê³¼
- ì‹¤ì œ ë°°í¬ ì‹œ ì„±ëŠ¥ í•˜ë½ ê°€ëŠ¥

### ìˆ˜ì • ì½”ë“œ (Data Leakage ì—†ìŒ)
- ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ Accuracy (ì˜ˆ: ~0.55-0.65)
- **ì •í™•í•œ** out-of-sample ì„±ëŠ¥
- ì‹¤ì œ ë°°í¬ ì‹œ ì˜ˆìƒ ì„±ëŠ¥ê³¼ ì¼ì¹˜

> **ì¤‘ìš”**: ì„±ëŠ¥ì´ ë‚®ì•„ì§€ëŠ” ê²ƒì´ ì •ìƒì…ë‹ˆë‹¤. ì´ê²ƒì´ ì‹¤ì œ ì‹œì¥ì—ì„œ ê¸°ëŒ€í•  ìˆ˜ ìˆëŠ” ì„±ëŠ¥ì…ë‹ˆë‹¤.

---

## ê²€ì¦ ë°©ë²•

### 1. Pipeline ë™ì‘ í™•ì¸
```python
# Pipelineì˜ ê° ë‹¨ê³„ë¥¼ í™•ì¸
fitted_pipeline = pipeline.fit(X_train, y_train)

# Scaler íŒŒë¼ë¯¸í„° í™•ì¸
print("Scaler mean:", fitted_pipeline.named_steps['scaler'].mean_)
print("Scaler std:", fitted_pipeline.named_steps['scaler'].scale_)

# ì„ íƒëœ features í™•ì¸ (feature selection ì‚¬ìš© ì‹œ)
selector = fitted_pipeline.named_steps['selector']
print("Selected features:", np.where(selector.get_support())[0])
```

### 2. CV Split í™•ì¸
```python
cv = WalkForwardCV(train_size=100, gap=10)
for train_idx, test_idx, should_refit in cv.split(X):
    print(f"Train: {train_idx[0]}~{train_idx[-1]}, "
          f"Test: {test_idx[0]}, Refit: {should_refit}")

    # Gap í™•ì¸
    assert train_idx[-1] + gap < test_idx[0]
```

### 3. Diebold-Mariano Test í•´ì„
```python
# DM Statistic > 0: Model 1ì´ Model 2ë³´ë‹¤ ë‚˜ì¨
# DM Statistic < 0: Model 1ì´ Model 2ë³´ë‹¤ ì¢‹ìŒ
# p-value < 0.05: í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì°¨ì´

dm_tests = perform_dm_tests(clf_results_after, y_true)
significant = dm_tests[dm_tests['Significant (5%)'] == True]
print(significant)
```

---

## ì¶”ê°€ ê°œì„  ê°€ëŠ¥ ì‚¬í•­

### 1. ë³‘ë ¬ ì²˜ë¦¬
```python
from joblib import Parallel, delayed

# CV loopë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
results = Parallel(n_jobs=-1)(
    delayed(train_one_fold)(pipeline, X, y, train_idx, test_idx)
    for train_idx, test_idx, _ in cv.split(X)
)
```

### 2. ëª¨ë¸ ìºì‹±
```python
import joblib

# í•™ìŠµëœ ëª¨ë¸ ì €ì¥
joblib.dump(pipeline, f'models/model_{iteration}.pkl')

# ë‚˜ì¤‘ì— ë¡œë“œ
pipeline = joblib.load(f'models/model_{iteration}.pkl')
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'classifier__n_estimators': [100, 500, 1000],
    'classifier__max_depth': [3, 5, 10],
    'selector__estimator__alpha': [0.001, 0.01, 0.1]
}

# Nested CVë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## ì°¸ê³  ìë£Œ

### Data Leakage ê´€ë ¨
- [Sklearn Pipeline Documentation](https://scikit-learn.org/stable/modules/compose.html)
- [Common Pitfalls in Time Series Cross-Validation](https://robjhyndman.com/hyndsight/tscv/)

### Walk-Forward Validation
- [Time Series Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html)
- Campisi et al. (2024) - Section 3.2: Validation Strategy

### Statistical Testing
- Diebold & Mariano (1995): "Comparing Predictive Accuracy"
- Harvey et al. (1997): "Testing the equality of prediction mean squared errors"

---

## ê²°ë¡ 

ì´ ìˆ˜ì • ë²„ì „ì€:
1. âœ… Data Leakage ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°
2. âœ… í†µê³„ì  ê²€ì •ì„ í†µí•´ ëª¨ë¸ ë¹„êµ ê°€ëŠ¥
3. âœ… ì„±ëŠ¥ ìµœì í™”ë¡œ ì‹¤ìš©ì„± í–¥ìƒ
4. âœ… ì •í™•í•œ out-of-sample ì„±ëŠ¥ ì¸¡ì •

**ì¶”ì²œ**: ì‹¤ì œ ë…¼ë¬¸ ì¬í˜„ ë° ì „ëµ ê°œë°œì—ëŠ” **ë°˜ë“œì‹œ ìˆ˜ì • ë²„ì „ì„ ì‚¬ìš©**í•˜ì„¸ìš”.
