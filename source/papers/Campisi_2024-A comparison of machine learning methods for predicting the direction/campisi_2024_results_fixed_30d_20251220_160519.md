# 머신러닝 기반 S&P 500 방향성 예측 성능 분석 (FIXED VERSION)

**생성일시**: 2025-12-20 17:18:20

**예측 기간**: 30일 후 방향성

**사용 방법론**: Volatility Indices 기반 머신러닝 모델 (Campisi et al., 2024)

**주요 개선사항**:
- ✅ Data Leakage 문제 해결 (Pipeline 사용)
- ✅ Diebold-Mariano 통계 검정 추가
- ✅ Performance 최적화 (Refit Frequency)

---

## 1. 데이터 요약

- **수집 기간**: 2011-01-03 ~ 2022-07-29
- **총 관측치 수**: 2,913
- **변수 수**: 10
- **Walk-Forward CV 이터레이션**: 755개
- **Refit Frequency**: 매 1일
- **데이터 소스**: Yahoo Finance

## 2. 기술 통계량

| Variable   |    N |    Mean |   St. Dev. |   Skewness |   Kurtosis |   rho1 |   rho2 |   rho3 |     ADF |         JB |
|:-----------|-----:|--------:|-----------:|-----------:|-----------:|-------:|-------:|-------:|--------:|-----------:|
| VIX        | 2913 |  18.145 |      7.353 |      2.532 |     11.379 |  0.967 |  0.945 |  0.921 |  -5.33  |  18767     |
| VIX9D      | 2913 |  17.56  |      8.801 |      3.294 |     19.505 |  0.939 |  0.908 |  0.87  |  -6.658 |  51273.3   |
| VIX3M      | 2913 |  20.037 |      6.572 |      1.979 |      7.007 |  0.981 |  0.968 |  0.951 |  -3.729 |   7834.16  |
| VIX6M      | 2913 |  21.419 |      5.879 |      1.395 |      2.607 |  0.987 |  0.977 |  0.965 |  -2.987 |   1764.74  |
| VVIX       | 2913 |  96.772 |     16.883 |      1.153 |      2.859 |  0.946 |  0.899 |  0.856 |  -5.954 |   1631.77  |
| SKEW       | 2913 | 128.957 |      9.666 |      0.763 |      0.295 |  0.928 |  0.896 |  0.866 |  -4.248 |    292.503 |
| VXN        | 2913 |  20.863 |      7.504 |      1.838 |      5.776 |  0.972 |  0.953 |  0.932 |  -4.768 |   5670.55  |
| GVZ        | 2913 |  17.144 |      4.955 |      1.284 |      3.126 |  0.976 |  0.954 |  0.934 |  -4.12  |   1980.05  |
| OVX        | 2913 |  37.335 |     18.956 |      5.082 |     44.54  |  0.966 |  0.93  |  0.907 |  -4.896 | 252462     |
| RVOL       | 2913 |  14.769 |      9.661 |      3.675 |     20.19  |  0.996 |  0.99  |  0.981 |  -6.667 |  55847     |
| Returns30  | 2913 |   0.012 |      0.051 |     -1.761 |      9.069 |  0.953 |  0.916 |  0.869 | -10.798 |  11447.1   |

## 3. VIF (Variance Inflation Factor)

| Variable   |      VIF |
|:-----------|---------:|
| VIX3M      | 2619.89  |
| VIX6M      | 1277.7   |
| VIX        | 1143.52  |
| VIX9D      |  211.145 |
| VXN        |  158.932 |
| VVIX       |  116.066 |
| SKEW       |   82.123 |
| GVZ        |   30.637 |
| RVOL       |   16.598 |
| OVX        |   14.972 |

## 4. 모델 성능 비교 (Feature Selection 전)

### 4.1 분류 모델

| Model                   |   Accuracy |    AUC |   F-measure |
|:------------------------|-----------:|-------:|------------:|
| Logistic Regression     |     0.6662 | 0.6131 |      0.7997 |
| LDA                     |     0.6675 | 0.6437 |      0.8006 |
| Random Forest (Clf)     |     0.6384 | 0.6373 |      0.7771 |
| Bagging (Clf)           |     0.6225 | 0.6416 |      0.7654 |
| Gradient Boosting (Clf) |     0.6053 | 0.6295 |      0.7508 |

### 4.2 회귀 모델

| Model                   |   Accuracy |    AUC |   F-measure |
|:------------------------|-----------:|-------:|------------:|
| Linear Regression       |     0.7113 | 0.6071 |      0.8313 |
| Ridge Regression        |     0.7126 | 0.6033 |      0.8322 |
| Lasso Regression        |     0.7245 | 0.5369 |      0.8402 |
| Random Forest (Reg)     |     0.6636 | 0.6218 |      0.7942 |
| Bagging (Reg)           |     0.6649 | 0.6218 |      0.7938 |
| Gradient Boosting (Reg) |     0.6609 | 0.561  |      0.7895 |

## 5. 모델 성능 비교 (Feature Selection 후)

### 5.1 분류 모델

| Model                   |   Accuracy |    AUC |   F-measure |
|:------------------------|-----------:|-------:|------------:|
| Logistic Regression     |     0.6675 | 0.6127 |      0.8006 |
| LDA                     |     0.6675 | 0.6337 |      0.8006 |
| Random Forest (Clf)     |     0.6464 | 0.6603 |      0.7831 |
| Bagging (Clf)           |     0.6371 | 0.6673 |      0.7765 |
| Gradient Boosting (Clf) |     0.6146 | 0.6041 |      0.7601 |

### 5.2 회귀 모델

| Model                   |   Accuracy |    AUC |   F-measure |
|:------------------------|-----------:|-------:|------------:|
| Linear Regression       |     0.7073 | 0.5597 |      0.8285 |
| Ridge Regression        |     0.7073 | 0.5596 |      0.8285 |
| Lasso Regression        |     0.7245 | 0.5369 |      0.8402 |
| Random Forest (Reg)     |     0.6146 | 0.6482 |      0.7593 |
| Bagging (Reg)           |     0.5801 | 0.6598 |      0.7311 |
| Gradient Boosting (Reg) |     0.6212 | 0.6023 |      0.7644 |

## 6. Diebold-Mariano 통계 검정

### 6.1 분류 모델 쌍별 비교

**통계적으로 유의한 차이 (p < 0.05):**

| Model 1             | Model 2                 |   DM Statistic |   p-value | Significant (5%)   |
|:--------------------|:------------------------|---------------:|----------:|:-------------------|
| Logistic Regression | Random Forest (Clf)     |        -2.2246 |    0.0261 | True               |
| Logistic Regression | Bagging (Clf)           |        -2.8664 |    0.0042 | True               |
| Logistic Regression | Gradient Boosting (Clf) |        -4.7148 |    0      | True               |
| LDA                 | Random Forest (Clf)     |        -2.2246 |    0.0261 | True               |
| LDA                 | Bagging (Clf)           |        -2.8664 |    0.0042 | True               |
| LDA                 | Gradient Boosting (Clf) |        -4.7148 |    0      | True               |
| Random Forest (Clf) | Gradient Boosting (Clf) |        -3.418  |    0.0006 | True               |
| Bagging (Clf)       | Gradient Boosting (Clf) |        -2.8075 |    0.005  | True               |

### 6.2 회귀 모델 쌍별 비교

**통계적으로 유의한 차이 (p < 0.05):**

| Model 1             | Model 2                 |   DM Statistic |   p-value | Significant (5%)   |
|:--------------------|:------------------------|---------------:|----------:|:-------------------|
| Linear Regression   | Lasso Regression        |         3.6346 |    0.0003 | True               |
| Linear Regression   | Random Forest (Reg)     |        -6.9431 |    0      | True               |
| Linear Regression   | Bagging (Reg)           |        -8.5537 |    0      | True               |
| Linear Regression   | Gradient Boosting (Reg) |        -6.5151 |    0      | True               |
| Ridge Regression    | Lasso Regression        |         3.6346 |    0.0003 | True               |
| Ridge Regression    | Random Forest (Reg)     |        -6.9431 |    0      | True               |
| Ridge Regression    | Bagging (Reg)           |        -8.5537 |    0      | True               |
| Ridge Regression    | Gradient Boosting (Reg) |        -6.5151 |    0      | True               |
| Lasso Regression    | Random Forest (Reg)     |        -8.7492 |    0      | True               |
| Lasso Regression    | Bagging (Reg)           |       -10.2351 |    0      | True               |
| Lasso Regression    | Gradient Boosting (Reg) |        -8.4082 |    0      | True               |
| Random Forest (Reg) | Bagging (Reg)           |        -4.5159 |    0      | True               |
| Bagging (Reg)       | Gradient Boosting (Reg) |         3.4695 |    0.0005 | True               |

## 7. 모델 성능 요약

### 7.1 Feature Selection 전후 비교

**분류 모델:**
- Feature Selection 전: LDA (Accuracy: 0.6675)
- Feature Selection 후: Logistic Regression (Accuracy: 0.6675)

**회귀 모델:**
- Feature Selection 전: Lasso Regression (Accuracy: 0.7245)
- Feature Selection 후: Lasso Regression (Accuracy: 0.7245)

### 7.2 전체 최고 성능 모델

**최고 성능**: Lasso Regression
- Accuracy: 0.7245
- AUC: 0.5369
- F-measure: 0.8402

## 8. 주요 인사이트

**1. Feature Selection 효과**
- 전체 평균 Accuracy: 0.6671 → 0.6535
- Feature Selection으로 성능 변화 (-0.0136)

**2. 분류 vs 회귀 모델**
- 분류 모델 평균 Accuracy: 0.6466
- 회귀 모델 평균 Accuracy: 0.6592
- 회귀 모델이 방향 예측에 더 효과적

**3. Data Leakage 문제 해결**
- ✅ Standardization을 CV loop 내부로 이동 (Pipeline 사용)
- ✅ Feature Selection을 CV loop 내부로 이동 (SelectFromModel)
- ✅ 이로 인해 원본 코드 대비 성능이 낮아질 수 있으나, 이것이 정확한 out-of-sample 성능임

---

## 9. 시각화

- `figures/correlation_heatmap.png`: 상관관계 히트맵
- `figures/roc_curves.png`: ROC 곡선
- `figures/returns_timeseries.png`: 수익률 시계열
