# Conclusion

Even though this book contains an abundance of strategies that should be interesting and attractive to independent or even institutional traders, it has not been a recipe of strategies, or a step-by-step guide to implementing them. The strategies described in this book serve only to illustrate the general technique or concept, but they are not guaranteed to be without those very pitfalls that I detailed in Chapter 1. Even if I were to carefully scrub them of pitfalls, good strategies can still be victims of **regime changes**. Readers are invited and encouraged to perform **out-of-sample testing** on the strategies in this book to see for themselves.

Instead of recipes, what I hope to convey is the deeper reasons, the **basic principles**, why certain strategies should work and why others shouldn't. Once we grasp the basic inefficiencies of certain markets (e.g., **regression to the mean**, the presence of **roll returns** in futures, the need for end-of-day rebalancing in **leveraged exchange-traded funds (ETFs)**), it is actually quite easy to come up with a strategy to exploit them. This notion of understanding the inefficiency first and constructing a strategy later is why I emphasized **simple and linear strategies**. Why create all kinds of arbitrary rules when the inefficiency can be exploited by a simple model?

The other notion I wanted to convey is that the approach to **algorithmic trading** can be rather scientific. In science, we form a **hypothesis**, express it as a **quantitative model**, and then test it against new, unseen data to see if the model is predictive. If the model failed with certain data, we try to find out the reasons for the failures, perhaps add certain variables to the model, and try again. This is a very similar process to how we should approach algorithmic trading. Recall the ETF pair **GLD versus GDX** that stopped

cointegrating in 2008 (see Chapter 4). A hypothesis was formed that had to do with the high crude oil price. When oil price was added to the input variables, the **cointegration model** started to work again. This scientific process is most helpful when a strategy underperforms the backtest, and we wanted to know why. Instead of blindly adding more rules, more indicators, to the model and hoping that they miraculously improve the model performance, we should look for a **fundamental reason** and then quantitatively test whether this fundamental reason is valid.

Despite the efforts to make the trading process scientific and rule based, there are still areas where **subjective judgment** is important. For example, when there is a major event looming, do you trust that your model will behave as your backtest predicted, or do you lower your leverage or even temporarily shut down the model in anticipation? Another example is offered by the application of the **Kelly formula** to a portfolio of strategies. Should we allocate capital among these strategies based on the equity of the whole portfolio, so that the good performance of some strategies is subsidizing the poor performance of others in the short term? Or should we apply the Kelly formula to each strategy on its own, so that we quickly deleverage those strategies that perform poorly recently? Mathematics tells us that the former solution is optimal, but that's assuming the **expected returns and volatilities** of the strategies are unchanging. Can one really say that such expectations are unchanged given a recent period of severe **drawdown**?

(On the first judgment call, my experience has been that if your model has survived the backtest during prior stressful periods, there is no reason to lower its leverage in the face of coming crisis. It is much better to start off with a more **conservative leverage** during good times than to have to lower it in bad ones. As Donald Rumsfeld once said, it is the **"unknown unknowns"** that will harm us, not the "known unknowns." Unfortunately, we can't shut down our models before the unknown unknowns strike. On the second judgment call, my experience has been that applying Kelly to each strategy independently so as to allow each one to wither and die quickly when it underperforms is more practical than applying **Kelly asset allocation** across all strategies.)

As these examples show, subjective judgment is often needed because the statistical properties of **financial time series** are not stationary, and science can really only deal with stationary statistics. (I am using *stationary* in a sense different from the stationarity of time series in Chapter 2. Here, it means the **probability distribution** of prices remains unchanged throughout time.) Often, when we find that our **live trading** experience diverges from the backtest, it is not because we committed any of the pitfalls during backtesting. It is because there has been a fundamental change in the **market structure**, a **regime shift**, due to government regulatory or macroeconomic changes. So the fund managers still have an active ongoing role even if the strategy is supposedly algorithmic and automatedâ€”their role is to make judicious high-level judgment calls based on their fundamental understanding of the markets on whether the models are still valid.

However, the fact that judgment is sometimes needed doesn't mean that developing **quantitative rules** is useless or algorithmic traders are less "smart" than **discretionary traders**. As the oft-quoted Daniel Kahneman wrote, experts are *uniformly inferior* to algorithms in every domain that has a significant degree of uncertainty or unpredictability, ranging from deciding winners of football games to predicting longevity of cancer patients. One can hope that the financial market is no exception to this rule.