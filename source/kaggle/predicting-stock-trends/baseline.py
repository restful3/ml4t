#!/usr/bin/env python3
"""
====================================================================
Kaggle ì£¼ì‹ íŠ¸ë Œë“œ ì˜ˆì¸¡ - ì˜¬ë°”ë¥¸ Baseline ëª¨ë¸
====================================================================

ëª©ì :
    ì£¼ì‹ì˜ ë§ˆì§€ë§‰ í•™ìŠµì¼ ì¢…ê°€ ëŒ€ë¹„ 30 ê±°ë˜ì¼ í›„ ê°€ê²©ì´
    ìƒìŠ¹(1) ë˜ëŠ” í•˜ë½(0)í• ì§€ ì˜ˆì¸¡

ëŒ€íšŒ ìš”êµ¬ì‚¬í•­:
    - test.csvì˜ ê° ì¢…ëª©ì— ëŒ€í•´
    - ë§ˆì§€ë§‰ í•™ìŠµì¼(2024-09-23) ì¢…ê°€ ëŒ€ë¹„
    - 30ì¼ í›„(2024-11-04) ì¢…ê°€ê°€ ë†’ì„ì§€(1) ë‚®ì„ì§€(0) ì˜ˆì¸¡

ì˜¬ë°”ë¥¸ ì ‘ê·¼:
    - ê³¼ê±° ë°ì´í„°ì—ì„œ ê° ì‹œì ì˜ "30ì¼ í›„ ì‹¤ì œ ê°€ê²©"ìœ¼ë¡œ íƒ€ê²Ÿ ìƒì„±
    - ê¸°ìˆ ì  ì§€í‘œë¡œ íŒ¨í„´ í•™ìŠµ
    - Random Forestë¡œ ë¶„ë¥˜

ì˜ˆìƒ ì„±ëŠ¥: 0.58~0.62 (ê¸°ì¡´ 0.50 ëŒ€ë¹„ í° ê°œì„ )

ì‘ì„±ì: ML4T Project
ë‚ ì§œ: 2025-11-06
====================================================================
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import warnings
from tqdm import tqdm
import os

warnings.filterwarnings('ignore')

# ====================================================================
# ì„¤ì •
# ====================================================================
DATA_DIR = 'data'
OUTPUT_DIR = 'outputs'
RANDOM_STATE = 42

# í•™ìŠµ ì„¤ì •
HORIZON = 30  # 30 ê±°ë˜ì¼ í›„ ì˜ˆì¸¡
LOOKBACK = 252  # ìµœê·¼ 1ë…„(252 ê±°ë˜ì¼) ë°ì´í„° ì‚¬ìš©

# ====================================================================
# 1. ë°ì´í„° ë¡œë”©
# ====================================================================

def load_data():
    """CSV íŒŒì¼ ë¡œë“œ"""
    print("ğŸ“‚ ë°ì´í„° ë¡œë”© ì¤‘...")

    train_df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))
    test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'))
    sample_submission = pd.read_csv(os.path.join(DATA_DIR, 'sample_submission.csv'))

    print(f"   Train: {train_df.shape}")
    print(f"   Test: {test_df.shape}")

    return train_df, test_df, sample_submission


# ====================================================================
# 2. íŠ¹ì§• ìƒì„±
# ====================================================================

def create_features(df):
    """
    OHLCV ë°ì´í„°ë¡œë¶€í„° ê¸°ìˆ ì  íŠ¹ì§• ìƒì„±

    íŠ¹ì§• (16ê°œ):
        - ê°€ê²© ê¸°ë°˜ (6ê°œ): returns, high_low_ratio, close_open_ratio, daily_range
        - ê±°ë˜ëŸ‰ (2ê°œ): volume, volume_log
        - ì´ë™í‰ê·  (3ê°œ): ma_5, ma_10, ma_20
        - ê±°ë˜ëŸ‰ MA (2ê°œ): volume_ma_5, volume_ma_10
        - ê°€ê²©/MA ë¹„ìœ¨ (2ê°œ): price_to_ma5, price_to_ma20
        - ê¸°ì—…í™œë™ (2ê°œ): has_dividend, has_split
    """
    features = pd.DataFrame()

    # ê°€ê²© ê¸°ë°˜ íŠ¹ì§•
    features['returns'] = (df['Close'] - df['Open']) / df['Open']
    features['high_low_ratio'] = df['High'] / df['Low']
    features['close_open_ratio'] = df['Close'] / df['Open']
    features['daily_range'] = (df['High'] - df['Low']) / df['Open']

    # ê±°ë˜ëŸ‰ íŠ¹ì§•
    features['volume'] = df['Volume']
    features['volume_log'] = np.log1p(df['Volume'])

    # ì´ë™í‰ê· 
    features['ma_5'] = df['Close'].rolling(window=5, min_periods=1).mean()
    features['ma_10'] = df['Close'].rolling(window=10, min_periods=1).mean()
    features['ma_20'] = df['Close'].rolling(window=20, min_periods=1).mean()

    # ê±°ë˜ëŸ‰ ì´ë™í‰ê· 
    features['volume_ma_5'] = df['Volume'].rolling(window=5, min_periods=1).mean()
    features['volume_ma_10'] = df['Volume'].rolling(window=10, min_periods=1).mean()

    # ê°€ê²©/MA ë¹„ìœ¨
    features['price_to_ma5'] = df['Close'] / features['ma_5']
    features['price_to_ma20'] = df['Close'] / features['ma_20']

    # ê¸°ì—… í™œë™
    features['has_dividend'] = (df['Dividends'] > 0).astype(int)
    features['has_split'] = (df['Stock Splits'] > 0).astype(int)

    # ë¬´í•œëŒ€/ê²°ì¸¡ê°’ ì²˜ë¦¬
    features = features.replace([np.inf, -np.inf], np.nan)
    features = features.fillna(0)

    return features


# ====================================================================
# 3. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
# ====================================================================

def prepare_training_data(train_df, horizon=HORIZON, lookback=LOOKBACK):
    """
    ì˜¬ë°”ë¥¸ ë°©ë²•ìœ¼ë¡œ í•™ìŠµ ë°ì´í„° ì¤€ë¹„

    ê° ì¢…ëª©ì˜ ê³¼ê±° ë°ì´í„°ì—ì„œ:
        - t ì‹œì  íŠ¹ì§• â†’ t+30ì¼ ì‹¤ì œ ê°€ê²© ìƒìŠ¹/í•˜ë½ì„ íƒ€ê²Ÿìœ¼ë¡œ í•™ìŠµ

    Args:
        train_df: í•™ìŠµ ë°ì´í„°
        horizon: ì˜ˆì¸¡ ê¸°ê°„ (30ì¼)
        lookback: ì‚¬ìš©í•  ê³¼ê±° ë°ì´í„° ê¸°ê°„ (252ì¼ = 1ë…„)

    Returns:
        X: íŠ¹ì§• DataFrame
        y: íƒ€ê²Ÿ ë°°ì—´ (0 ë˜ëŠ” 1)
    """
    print("\nğŸ“Š í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    print(f"   - ì˜ˆì¸¡ ê¸°ê°„: {horizon} ê±°ë˜ì¼")
    print(f"   - ì‚¬ìš© ê¸°ê°„: ìµœê·¼ {lookback} ê±°ë˜ì¼")

    all_features = []
    all_targets = []

    tickers = train_df['Ticker'].unique()

    for ticker in tqdm(tickers, desc="ì¢…ëª© ì²˜ë¦¬ ì¤‘"):
        # í•´ë‹¹ ì¢…ëª© ë°ì´í„°
        ticker_data = train_df[train_df['Ticker'] == ticker].copy()
        ticker_data = ticker_data.sort_values('Date').reset_index(drop=True)

        # ìµœì†Œ ë°ì´í„° ì²´í¬
        if len(ticker_data) < lookback + horizon:
            continue

        # ìµœê·¼ lookback + horizonì¼ ë°ì´í„°ë§Œ ì‚¬ìš©
        recent_data = ticker_data.iloc[-(lookback + horizon):].reset_index(drop=True)

        # íŠ¹ì§• ìƒì„± (ì „ì²´ ë°ì´í„°ë¡œ)
        features = create_features(recent_data)

        # ê° ì‹œì ë§ˆë‹¤ ìƒ˜í”Œ ìƒì„±
        for i in range(len(recent_data) - horizon):
            # í˜„ì¬ ì‹œì  ì¢…ê°€
            current_close = recent_data.iloc[i]['Close']

            # 30ì¼ í›„ ì‹¤ì œ ì¢…ê°€
            future_close = recent_data.iloc[i + horizon]['Close']

            # íƒ€ê²Ÿ: 30ì¼ í›„ > í˜„ì¬?
            target = 1 if future_close > current_close else 0

            # í˜„ì¬ ì‹œì  íŠ¹ì§•
            current_features = features.iloc[i]

            all_features.append(current_features)
            all_targets.append(target)

    # DataFrameìœ¼ë¡œ ë³€í™˜
    X = pd.DataFrame(all_features).reset_index(drop=True)
    y = np.array(all_targets)

    print(f"\nâœ… í•™ìŠµ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    print(f"   - ì´ ìƒ˜í”Œ: {len(X):,}ê°œ")
    print(f"   - ìƒìŠ¹(1): {y.sum():,}ê°œ ({y.mean():.1%})")
    print(f"   - í•˜ë½(0): {len(y) - y.sum():,}ê°œ ({1-y.mean():.1%})")

    return X, y


# ====================================================================
# 4. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
# ====================================================================

def prepare_test_features(test_df, train_df):
    """
    í…ŒìŠ¤íŠ¸ ë°ì´í„° íŠ¹ì§• ì¤€ë¹„

    ê° ì¢…ëª©ì˜ ë§ˆì§€ë§‰ í•™ìŠµì¼(2024-09-23) ë°ì´í„°ë¡œ íŠ¹ì§• ìƒì„±

    Args:
        test_df: í…ŒìŠ¤íŠ¸ ë°ì´í„° (ID, Date)
        train_df: í•™ìŠµ ë°ì´í„°

    Returns:
        test_features: í…ŒìŠ¤íŠ¸ íŠ¹ì§• DataFrame
        test_tickers: ì¢…ëª© ë¦¬ìŠ¤íŠ¸
    """
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

    test_features = []
    test_tickers = []

    for _, row in tqdm(test_df.iterrows(), total=len(test_df), desc="í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì²˜ë¦¬ ì¤‘"):
        ticker = row['ID']

        # í•´ë‹¹ ì¢…ëª©ì˜ í•™ìŠµ ë°ì´í„°
        ticker_train = train_df[train_df['Ticker'] == ticker].copy()
        ticker_train = ticker_train.sort_values('Date')

        if len(ticker_train) > 0:
            # ìµœê·¼ ë°ì´í„°ë¡œ íŠ¹ì§• ìƒì„± (ì´ë™í‰ê·  ê³„ì‚° ìœ„í•´ ì¶©ë¶„í•œ ë°ì´í„° í•„ìš”)
            recent_data = ticker_train.iloc[-30:] if len(ticker_train) >= 30 else ticker_train

            # íŠ¹ì§• ìƒì„±
            features = create_features(recent_data)

            # ë§ˆì§€ë§‰ í–‰ (ê°€ì¥ ìµœê·¼ ë°ì´í„°)
            last_features = features.iloc[-1]

            test_features.append(last_features)
            test_tickers.append(ticker)
        else:
            # ë°ì´í„° ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ì›€
            test_features.append(pd.Series(0, index=range(16)))
            test_tickers.append(ticker)

    test_X = pd.DataFrame(test_features).reset_index(drop=True)

    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(test_X)}ê°œ ìƒ˜í”Œ")

    return test_X, test_tickers


# ====================================================================
# 5. ëª¨ë¸ í•™ìŠµ
# ====================================================================

def train_model(X_train, y_train):
    """
    Random Forest ëª¨ë¸ í•™ìŠµ

    í•˜ì´í¼íŒŒë¼ë¯¸í„°:
        - n_estimators: 200 (ë” ë§ì€ íŠ¸ë¦¬)
        - max_depth: 15 (ë” ê¹Šê²Œ)
        - min_samples_split: 10 (ë” ì„¸ë°€í•˜ê²Œ)
        - min_samples_leaf: 5
        - random_state: 42
    """
    print("\nğŸŒ² Random Forest ëª¨ë¸ í•™ìŠµ ì¤‘...")

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=15,
        min_samples_split=10,
        min_samples_leaf=5,
        random_state=RANDOM_STATE,
        n_jobs=-1,
        verbose=1
    )

    model.fit(X_train, y_train)

    print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

    return model


# ====================================================================
# 6. ë©”ì¸ ì‹¤í–‰
# ====================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "=" * 60)
    print("ğŸ¯ ì£¼ì‹ íŠ¸ë Œë“œ ì˜ˆì¸¡ - ì˜¬ë°”ë¥¸ Baseline ëª¨ë¸")
    print("=" * 60)

    # 1. ë°ì´í„° ë¡œë”©
    train_df, test_df, sample_submission = load_data()

    # 2. í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    X, y = prepare_training_data(train_df, horizon=HORIZON, lookback=LOOKBACK)

    # 3. Train/Validation ë¶„í• 
    print("\nğŸ“Š ë°ì´í„° ë¶„í•  ì¤‘...")
    X_train, X_val, y_train, y_val = train_test_split(
        X, y,
        test_size=0.2,
        random_state=RANDOM_STATE,
        stratify=y
    )

    print(f"   - Train: {len(X_train):,}ê°œ")
    print(f"   - Validation: {len(X_val):,}ê°œ")

    # 4. ìŠ¤ì¼€ì¼ë§
    print("\nğŸ“ ë°ì´í„° ì •ê·œí™” ì¤‘...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)

    # 5. ëª¨ë¸ í•™ìŠµ
    model = train_model(X_train_scaled, y_train)

    # 6. Validation í‰ê°€
    print("\nğŸ“ˆ Validation ì„±ëŠ¥ í‰ê°€...")
    y_val_pred = model.predict(X_val_scaled)
    val_accuracy = accuracy_score(y_val, y_val_pred)

    print(f"\n{'='*60}")
    print(f"âœ¨ Validation Accuracy: {val_accuracy:.4f}")
    print(f"{'='*60}")

    print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    print(classification_report(y_val, y_val_pred, target_names=['í•˜ë½(0)', 'ìƒìŠ¹(1)']))

    # 7. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    test_X, test_tickers = prepare_test_features(test_df, train_df)

    # 8. ìŠ¤ì¼€ì¼ë§
    test_X_scaled = scaler.transform(test_X)

    # 9. ì˜ˆì¸¡
    print("\nğŸ”® í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
    test_predictions = model.predict(test_X_scaled)

    # 10. ì œì¶œ íŒŒì¼ ìƒì„±
    submission = sample_submission.copy()
    submission['Pred'] = test_predictions

    output_path = os.path.join(OUTPUT_DIR, 'submission_baseline_v3.csv')
    submission.to_csv(output_path, index=False)

    print(f"\nâœ… ì œì¶œ íŒŒì¼ ì €ì¥: {output_path}")
    print(f"\nì˜ˆì¸¡ ë¶„í¬:")
    print(f"   - ìƒìŠ¹(1): {test_predictions.sum()}ê°œ ({test_predictions.mean():.1%})")
    print(f"   - í•˜ë½(0): {len(test_predictions) - test_predictions.sum()}ê°œ ({1-test_predictions.mean():.1%})")

    # 11. íŠ¹ì§• ì¤‘ìš”ë„
    print("\nğŸ“Š Top 10 ì¤‘ìš” íŠ¹ì§•:")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)

    for idx, row in feature_importance.head(10).iterrows():
        print(f"   {row['feature']:20s}: {row['importance']:.4f}")

    print("\n" + "=" * 60)
    print("ğŸ‰ ì™„ë£Œ!")
    print("=" * 60)
    print(f"\nğŸ’¡ ì˜ˆìƒ ì„±ëŠ¥: 0.58~0.62 (ê¸°ì¡´ 0.50 ëŒ€ë¹„ í° ê°œì„ )")
    print(f"   - Validation: {val_accuracy:.4f}")
    print(f"   - ì œì¶œ íŒŒì¼: {output_path}")
    print()


if __name__ == "__main__":
    main()
