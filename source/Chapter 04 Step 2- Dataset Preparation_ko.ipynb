{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 Step 2: 데이터셋 준비\n",
    "\n",
    "문제가 정의되면 다음 단계는 예측 모델을 학습하고 테스트하는 데 사용할 관련 과거 데이터를 수집하고 전처리하는 것입니다. 데이터셋의 품질과 포괄성은 모델의 성능과 보지 못한 데이터에 대해 일반화할 수 있는 능력에 직접적인 영향을 미칩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 수집\n",
    "\n",
    "데이터셋 준비의 첫 번째 단계는 데이터 수집으로, 이는 해당 자산의 과거 가격 데이터, 거래량 및 기타 관련 시장 데이터를 수집하는 것을 포함합니다. 추가적으로, 거시경제 지표, 기업 재무제표, 뉴스 기사나 소셜 미디어의 감성 분석과 같은 대체 데이터 소스도 유용한 인사이트를 제공할 수 있습니다. 데이터의 정확성과 무결성을 유지하기 위해 신뢰할 수 있는 공급자로부터 데이터를 확보하는 것이 매우 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 탐색적 데이터 분석 (Exploratory Data Analysis)\n",
    "\n",
    "데이터가 수집된 후에는 데이터셋과 그 특성을 이해하는 과정이 필요합니다. 탐색적 데이터 분석(EDA)은 요약 통계와 차트를 활용하여 데이터의 패턴을 발견하고, 이상치를 탐지하며, 기존의 가정을 검증하는 분석 및 시각화 과정입니다. 이는 데이터 전처리 및 모델 설계의 다음 단계를 결정하는 데에도 도움을 줍니다.\n",
    "\n",
    "Python에서는 `pandas` (참고: [qnt.co/book-pandas](https://qnt.co/book-pandas))와 같은 라이브러리를 통해 데이터를 효율적으로 조작할 수 있으며, `Sweetviz` (참고: [qnt.co/book-sweetviz](https://qnt.co/book-sweetviz))와 같은 도구를 활용하면 자동화된 EDA 리포트를 생성할 수 있습니다.\n",
    "\n",
    "Sweetviz를 설치하려면 아래 명령어를 실행하세요.\n",
    "\n",
    "```bash\n",
    "pip install sweetviz\n",
    "```\n",
    "\n",
    "아래는 `Age`, `Income`, `Gender`, `City`로 구성된 데이터셋을 예시로 Sweetviz를 사용하는 방법입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sweetviz as sv\n",
    "np.random.seed(42)\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "data = {\n",
    "    'Age': np.random.randint(8, 90, size=100),\n",
    "    'Income': np.random.randint(10000, 500000, size=100),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], size=100),\n",
    "    'City': np.random.choice(['New York', 'Singapore', 'Paris', 'Rome', 'Tokyo'], size=100)\n",
    "}\n",
    "\n",
    "# DataFrame 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 일반 리포트 생성\n",
    "general_report = sv.analyze(df)\n",
    "general_report.show_html('sweetviz_report.html')\n",
    "\n",
    "# 성별 기준 비교 리포트 생성\n",
    "gender_report = sv.compare_intra(df, df['Gender'] == 'Male', ['Male', 'Female'])\n",
    "gender_report.show_html('sweetviz_gender_comparison.html')\n",
    "```\n",
    "\n",
    "위 코드는 먼저 데이터셋에 대한 전반적인 인터랙티브 리포트를 생성하여 HTML 파일로 저장합니다 (그림 4.1).\n",
    "\n",
    "<img src=\"./images/fig_04_01.png\" width=800>\n",
    "\n",
    "그림 4.1 Sweetviz 대시보드 – 비교 대상 없음.\n",
    "\n",
    "또한 `compare_intra` 메서드를 사용하여 성별 기준으로 데이터를 나누어 비교 리포트를 생성합니다 (그림 4.2).\n",
    "\n",
    "<img src=\"./images/fig_04_02.png\" width=800>\n",
    "\n",
    "그림 4.2 Sweetviz 대시보드 – 성별을 비교 대상으로 사용.\n",
    "\n",
    "전체 문서는 [gmt.co/book-sweetviz-docs](https://gmt.co/book-sweetviz-docs)에서 확인할 수 있습니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 (Data Preprocessing)\n",
    "\n",
    "입력 데이터의 특성을 이해한 후에는 전처리 단계로 넘어갑니다. 이 단계에서는 결측치, 이상값, 중복 제거, 오류 또는 불일치 수정 등 데이터에 존재할 수 있는 문제들을 정제(cleaning)합니다. 이러한 데이터 문제는 데이터 입력 오류, 기술적 문제, 시장 이상 현상, 갑작스러운 이벤트 등 다양한 원인으로 발생할 수 있습니다.\n",
    "\n",
    "또한 변수 간 스케일이 다를 경우, **정규화(normalization)** 또는 **표준화(standardization)** 작업이 필요할 수 있습니다. 모든 변수를 동일한 스케일로 변환하면 특정 머신러닝 알고리즘의 성능 및 수렴 속도를 개선할 수 있습니다.\n",
    "\n",
    "시계열 분석에서는 시계열 데이터를 **정상성(stationary)** 을 갖도록 변환하는 것이 모델의 정확성과 신뢰성을 높이는 데 중요합니다. 시계열의 통계적 특성이 시간에 따라 일정하게 유지되도록 해야 합니다. 또한, **Engle-Granger 검정**은 시계열 간 공적분 관계를 식별하는 데 사용되며, 이는 평균회귀 기반 페어 트레이딩 전략을 개발할 때 필수적입니다.\n",
    "\n",
    "**결측치 처리 (Handling Missing Data)**\n",
    "\n",
    "결측치 처리(handling missing data)는 데이터 전처리 파이프라인에서 매우 중요한 단계입니다. 특히 금융 AI 응용에서는 결측 데이터가 결과를 왜곡하고, 모델 정확도를 떨어뜨리며, 잘못된 금융 예측을 초래할 수 있습니다.\n",
    "\n",
    "**결측치 식별 (Identifying Missing Data)**\n",
    "\n",
    "결측치를 처리하기 전에는 어디에, 얼마나 결측치가 존재하는지를 파악하는 것이 중요합니다. 아래는 Python의 `pandas`를 이용한 예시입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 재현 가능성을 위한 시드 설정\n",
    "np.random.seed(42)\n",
    "\n",
    "# 금융 데이터 생성\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'Date': pd.date_range(start='1/1/2023', periods=n_samples, freq='D'),\n",
    "    'Open': np.random.uniform(100, 500, size=n_samples),\n",
    "    'High': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Low': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Close': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Volume': np.random.randint(1000, 100000, size=n_samples)\n",
    "}\n",
    "\n",
    "# Volume을 float형으로 변환\n",
    "data['Volume'] = data['Volume'].astype(float)\n",
    "\n",
    "# 데모용 결측값 추가\n",
    "data['Volume'][np.random.choice(n_samples, size=50, replace=False)] = np.nan\n",
    "data['Close'][np.random.choice(n_samples, size=20, replace=False)] = np.nan\n",
    "\n",
    "# DataFrame 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 결측치 확인\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "```\n",
    "\n",
    "**결측치 처리 전략 (Strategies for Handling Missing Data)**\n",
    "\n",
    "1. 결측값 제거: 가장 간단한 접근 방식이지만, 데이터 손실이 클 수 있습니다.\n",
    "\n",
    "    ```python\n",
    "    # 결측값이 있는 행 제거\n",
    "    df_cleaned = df.dropna()\n",
    "    # 결측값이 있는 열 제거\n",
    "    df_cleaned = df.dropna(axis=1)\n",
    "    ```\n",
    "\n",
    "2. 대체(Imputation): 통계적 방법이나 머신러닝 모델을 이용해 결측값을 채우는 방식입니다. 모든 예시에서 `Date` 열은 수치형이 아니므로 대체에서 제외해야 합니다.\n",
    "\n",
    "* a. 평균/중앙값/최빈값 대체: 각 열의 평균, 중앙값 또는 최빈값을 사용한 단순 대체 방법입니다.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    # 평균을 이용한 대체\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df.iloc[:, 1:] = imputer.fit_transform(df.iloc[:, 1:])\n",
    "    ```\n",
    "\n",
    "* b. K-최근접 이웃(KNN) 대체: K개의 최근접 이웃의 값을 사용하여 결측값을 대체합니다.\n",
    "\n",
    "```python\n",
    "    from sklearn.impute import KNNImputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    df.iloc[:, 1:] = imputer.fit_transform(df.iloc[:, 1:])\n",
    "```\n",
    "\n",
    "* c. 다변량 연쇄 회귀 대체(MICE): 회귀 모델을 연쇄적으로 적용하여 여러 번 대체를 수행합니다.\n",
    "\n",
    "```python\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    imputer = IterativeImputer()\n",
    "    df.iloc[:, 1:] = imputer.fit_transform(df.iloc[:, 1:])\n",
    "```\n",
    "\n",
    "**대체 결과 평가 (Evaluating Imputation)**\n",
    "\n",
    "결측치를 처리한 후에는 시각화나 통계적 방법을 통해 원본 데이터와 대체된 데이터셋을 비교하여 대체가 데이터셋에 끼친 영향을 평가해야 합니다.\n",
    "\n",
    "**이상치 처리 (Handling Outliers)**\n",
    "\n",
    "금융 데이터의 이상치는 예측 모델의 결과를 심각하게 왜곡시키고 잘못된 결론을 유도할 수 있습니다.\n",
    "\n",
    "**이상치 식별 (Identifying Outliers)**\n",
    "\n",
    "이상치를 처리하기 위한 첫 단계는 이상치를 식별하는 것입니다. 일반적인 이상치 탐지 기법에는 박스 플롯(box plot), 산점도(scatterplot)와 같은 시각화 방법과 z-점수(z-score), 사분위 범위(IQR)와 같은 통계적 방법이 있습니다.\n",
    "\n",
    "**박스 플롯(Box Plot)을 이용한 이상치 식별**\n",
    "\n",
    "박스 플롯(그림 4.3)은 데이터 분포의 시각적 요약을 제공하며, 이상치의 존재를 쉽게 파악할 수 있게 합니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500, 103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 박스 플롯\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(df['Price'])\n",
    "plt.title('Box Plot for Price')\n",
    "plt.ylabel('Price')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_04_03.png\" width=800>\n",
    "\n",
    "그림 4.3 박스 플롯을 통한 이상치 식별\n",
    "그림 4.3에서 500이라는 값이 이상치로 식별됩니다.\n",
    "\n",
    "**Z-점수(Z-score)를 이용한 이상치 식별 (Using Z-score to Identify Outliers)**\n",
    "\n",
    "Z-점수는 각 데이터가 평균으로부터 몇 표준편차 떨어져 있는지를 나타냅니다. 일반적으로 Z-점수가 2보다 크거나 -2보다 작은 값(또는 더 엄격하게는 ±3 이상인 경우)을 이상치로 간주합니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500,103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "# Z-점수 계산\n",
    "df['Z_score'] = (df['Price'] - df['Price'].mean()) / df['Price'].std()\n",
    "# 이상치 식별\n",
    "outliers = df[np.abs(df['Z_score']) > 2]\n",
    "print(\"Z-점수 방법을 사용한 이상치:\")\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "Z-점수 방법을 사용한 이상치:\n",
    "    Price Z_score\n",
    "7 500 2.844444\n",
    "```\n",
    "\n",
    "**사분위 범위(IQR)를 이용한 이상치 식별 (Using Interquartile Range (IQR) to Identify Outliers)**\n",
    "\n",
    "IQR은 제1사분위수(25번째 백분위수)와 제3사분위수(75번째 백분위수) 사이의 범위입니다. 이상치는 일반적으로 IQR의 1.5배를 초과하여 제3사분위수보다 크거나, 제1사분위수보다 작은 값을 갖는 데이터 포인트로 정의됩니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500, 103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# IQR 계산\n",
    "Q1 = df['Price'].quantile(0.25)\n",
    "Q3 = df['Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# 이상치 식별\n",
    "outliers = df[(df['Price'] < (Q1 - 1.5 * IQR)) | (df['Price'] > (Q3 + 1.5 * IQR))]\n",
    "print(\"IQR 방법을 사용한 이상치:\")\n",
    "print(outliers)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "IQR 방법을 사용한 이상치:\n",
    "    Price\n",
    "7   500\n",
    "```\n",
    "\n",
    "**이상치 처리 (Handling Outliers)**\n",
    "\n",
    "이상치를 식별한 후에는 다음과 같은 여러 가지 전략으로 처리할 수 있습니다. 여기에는 이상치 제거, 변환, 상한/하한 설정(capping/flooring) 등이 포함됩니다.\n",
    "\n",
    "1. 이상치 제거: 데이터 입력 오류로 인한 이상치일 경우 제거하는 것이 간단한 방법입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500, 103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Z-점수 계산\n",
    "df['Z_score'] = (df['Price'] - df['Price'].mean()) / df['Price'].std()\n",
    "\n",
    "# Z-점수를 이용한 이상치 제거\n",
    "df_cleaned = df[np.abs(df['Z_score']) <= 2]\n",
    "print(\"이상치 제거 후 데이터:\")\n",
    "print(df_cleaned)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "이상치 제거 후 데이터:\n",
    "    Price Z_score\n",
    "0 100 -0.320445\n",
    "1 95 -0.360006\n",
    "2 96 -0.352094\n",
    "3 101 -0.312533\n",
    "4 103 -0.296708\n",
    "5 98 -0.336269\n",
    "6 99 -0.328357\n",
    "8 103 -0.296708\n",
    "9 110 -0.241323\n",
    "```\n",
    "\n",
    "2. 이상치 변환: 이상치가 시장의 중요한 이벤트를 반영한 유효한 관측값일 경우, 변환을 통해 영향을 줄일 수 있습니다. 아래는 로그 변환 예시입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500,103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 로그 변환을 통해 이상치 영향 감소\n",
    "df['Price_log'] = np.log(df['Price'])\n",
    "print(\"로그 변환 후 데이터:\")\n",
    "print(df)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "로그 변환 후 데이터:\n",
    "    Price Price_log\n",
    "0 100 4.605170\n",
    "1 95 4.553877\n",
    "2 96 4.564348\n",
    "3 101 4.615121\n",
    "4 103 4.634729\n",
    "5 98 4.584967\n",
    "6 99 4.595120\n",
    "7 500 6.214608 \n",
    "8 103 4.634729 \n",
    "9 110 4.700480 \n",
    "```\n",
    "\n",
    "3. 이상치 상한/하한 설정 (Capping/Flooring): 이상치 값을 특정 임계값으로 제한하여 영향력을 줄이는 방법입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 샘플 금융 데이터\n",
    "data = {'Price': [100, 95, 96, 101, 103, 98, 99, 500,103, 110]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# IQR 계산\n",
    "Q1 = df['Price'].quantile(0.25)\n",
    "Q3 = df['Price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# IQR을 이용한 이상치 상한 설정\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df['Price_capped'] = np.where(df['Price'] > upper_bound, upper_bound, np.where(df['Price'] < lower_bound, lower_bound, df['Price']))\n",
    "print(\"이상치 상한 설정 후 데이터:\")\n",
    "print(df)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "이상치 상한 설정 후 데이터:\n",
    "    Price Price_capped\n",
    "0 100 100.000\n",
    "1 95 95.000\n",
    "2 96 96.000\n",
    "3 101 101.000\n",
    "4 103 103.000\n",
    "5 98 98.000\n",
    "6 99 99.000\n",
    "7 500 110.125\n",
    "8 103 103.000 \n",
    "9 110 110.000 \n",
    "```\n",
    "\n",
    "**특성 공학 (Feature Engineering)**\n",
    "\n",
    "특성 공학은 머신러닝 모델의 성능을 향상시키기 위해 원시(raw) 데이터를 알고리즘이 보다 잘 이해할 수 있는 형태로 변환하거나, 새로운 특성을 생성하는 작업입니다. 이를 통해 예측력을 높일 수 있습니다.\n",
    "\n",
    "특성 공학 기법에는 정규화(normalization), 표준화(standardization), 범주형 변수 인코딩, 결측치 처리, 상호작용 항(interaction terms) 생성 등이 포함됩니다.\n",
    "\n",
    "아래는 이동 평균(Moving Average) 특성 두 개를 생성하는 예시입니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 재현성을 위한 시드 설정\n",
    "np.random.seed(42)\n",
    "\n",
    "# 샘플 금융 데이터 생성\n",
    "n_samples = 1000\n",
    "data = {\n",
    "    'Date': pd.date_range(start='1/1/2023', periods=n_samples, freq='D'),\n",
    "    'Open': np.random.uniform(100, 500, size=n_samples),\n",
    "    'High': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Low': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Close': np.random.uniform(100, 500, size=n_samples),\n",
    "    'Volume': np.random.randint(1000, 100000, size=n_samples)\n",
    "}\n",
    "\n",
    "# DataFrame 생성\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 이동 평균 계산\n",
    "df['MA5'] = df['Close'].rolling(window=5).mean()    # 5일 이동 평균\n",
    "df['MA10'] = df['Close'].rolling(window=10).mean()  # 10일 이동 평균\n",
    "```\n",
    "\n",
    "**특성의 정규화 및 표준화 (Normalization and Standardization of Features)**\n",
    "\n",
    "정규화와 표준화는 데이터를 동일한 스케일로 변환하여 비교와 분석을 용이하게 만듭니다. 특히 서로 다른 단위나 범위를 가진 특성들을 사용할 때 머신러닝 알고리즘의 성능과 수렴 속도를 향상시키는 데 매우 유용합니다.\n",
    "\n",
    "\n",
    "**정규화 (Normalization)**\n",
    "\n",
    "정규화는 Min-Max 스케일링이라고도 하며, 데이터를 보통 0과 1 사이의 범위로 변환합니다. 이는 특성의 최소값을 빼고, 최대값과 최소값의 차이로 나누는 방식입니다.\n",
    "\n",
    "정규화 공식:\n",
    "\n",
    "$$\n",
    "X_{\\text {norm }}=\\frac{X-X_{\\text {min }}}{X_{\\text {max }}-X_{\\text {min }}}\n",
    "$$\n",
    "\n",
    "정규화는 데이터 분포를 가정하지 않는 신경망(Neural Networks)이나 K-최근접 이웃(K-Nearest Neighbors)과 같은 알고리즘에 특히 유용합니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 샘플 데이터\n",
    "data = {\n",
    "    'Feature1': [.4, .2, .1, .9, .6],\n",
    "    'Feature2': [90, 101, 95, 94, 102],\n",
    "    'Feature3': [9000, 10100, 9500, 9400, 10200]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# MinMaxScaler 초기화\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 데이터에 정규화 적용\n",
    "normalized_data = scaler.fit_transform(df)\n",
    "\n",
    "# 정규화된 데이터를 DataFrame으로 변환\n",
    "normalized_df = pd.DataFrame(normalized_data, columns=df.columns)\n",
    "\n",
    "# 정규화된 DataFrame 출력\n",
    "print(\"정규화된 DataFrame:\")\n",
    "print(normalized_df)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "정규화된 DataFrame:\n",
    "    Feature1  Feature2  Feature3\n",
    "0     0.375  0.000000  0.000000\n",
    "1     0.125  0.916667  0.916667\n",
    "2     0.000  0.416667  0.416667\n",
    "3     1.000  0.333333  0.333333\n",
    "4     0.625  1.000000  1.000000\n",
    "```\n",
    "\n",
    "**표준화 (Standardization)**\n",
    "\n",
    "금융 데이터셋에는 다양한 스케일과 단위를 가진 특성들이 포함되어 있는 경우가 많습니다. 예를 들어, 주가는 몇 달러에서 수천 달러까지 다양하고, 일간 수익률은 아주 작으며, 거래량은 수백만 단위일 수 있습니다. 이러한 차이는 입력 데이터의 스케일에 민감한 특정 머신러닝 알고리즘(예: 서포트 벡터 머신)의 성능에 부정적인 영향을 미칠 수 있습니다.\n",
    "\n",
    "표준화(또는 Z-점수 정규화)의 주된 목적은 모든 특성이 모델에 동일하게 기여하도록 평균이 0이고 표준편차가 1인 스케일로 변환하는 것입니다.\n",
    "\n",
    "표준화의 수학적 공식은 다음과 같습니다:\n",
    "\n",
    "$$\n",
    "Z=\\frac{X-\\mu}{\\sigma}\n",
    "$$\n",
    "\n",
    "여기서:\n",
    "\n",
    "- $Z$는 표준화된 값입니다.  \n",
    "- $X$는 원래 특성값입니다.  \n",
    "- $\\mu$는 평균으로 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "\\mu=\\frac{1}{N} \\sum_{i=1}^{N} X_{i}\n",
    "$$\n",
    "\n",
    "$N$은 데이터 포인트의 수이며, $X_{i}$는 $i$번째 데이터 포인트의 특성 값입니다.\n",
    "\n",
    "- $\\sigma$는 표준편차로 다음과 같이 정의됩니다:\n",
    "\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2}\n",
    "$$\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 샘플 데이터\n",
    "data = {\n",
    "    'Feature1': [.4, .2, .1, .9, .6],\n",
    "    'Feature2': [90, 101, 95, 94, 102],\n",
    "    'Feature3': [9000, 10100, 9500, 9400, 10200]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# StandardScaler 초기화\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 데이터에 적합 및 변환\n",
    "standardized_data = scaler.fit_transform(df)\n",
    "\n",
    "# DataFrame으로 변환\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=df.columns)\n",
    "\n",
    "# 표준화된 DataFrame 출력\n",
    "print(\"Standardized DataFrame:\")\n",
    "print(standardized_df)\n",
    "```\n",
    "\n",
    "출력 결과\n",
    "\n",
    "```\n",
    "Standardized DataFrame:\n",
    "Feature1 Feature2 Feature3\n",
    "0 -0.139347 -1.422574 -1.422574\n",
    "1 -0.836080  1.022475  1.022475\n",
    "2 -1.184446 -0.311188 -0.311188\n",
    "3  1.602486 -0.533465 -0.533465\n",
    "4  0.557386  1.244752  1.244752\n",
    "```\n",
    "\n",
    "**시계열 특성의 정상화 변환 (Transforming Time Series Features to Stationary)**\n",
    "\n",
    "시계열 분석의 주요 목표 중 하나는 데이터가 시간에 따라 일관된 통계적 특성을 갖도록 하는 것입니다. 평균, 분산, 자기상관 등 통계적 특성이 시간에 따라 일정한 시계열을 **정상 시계열(Stationary Time Series)** 이라고 합니다. 비정상(non-stationary) 시계열은 패턴 인식 및 예측 정확도가 낮아질 수 있습니다.\n",
    "\n",
    "정상성을 달성하기 위한 대표적인 방법으로는 차분(differencing), 추세 제거(detrending), 로그 변환(log transformation) 등이 있습니다.\n",
    "\n",
    "**확장 디키-풀러 검정 (ADF Test)** 은 정상성 여부를 판단하는 데 널리 사용되는 방법입니다. 이 테스트는 단위근(unit root)이 있는지를 검정하며, 검정 통계량이 임계값보다 작으면 단위근이 없다고 판단하여 정상 시계열로 간주합니다.\n",
    "\n",
    "아래는 ADF 테스트와 차분을 사용하는 Python 예시입니다:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 비정상 시계열 생성\n",
    "np.random.seed(42)\n",
    "time_series = np.random.randn(100).cumsum()\n",
    "\n",
    "# ADF 테스트 수행\n",
    "result = adfuller(time_series)\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "for key, value in result[4].items():\n",
    "    print('Critical Values:')\n",
    "    print(f' {key}, {value}')\n",
    "\n",
    "# 원본 시계열 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_series, label='Original Time Series')\n",
    "plt.title('Non-Stationary Time Series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 차분 적용\n",
    "diff_series = np.diff(time_series, n=1)\n",
    "\n",
    "# 차분된 시계열 ADF 테스트\n",
    "result_diff = adfuller(diff_series)\n",
    "print('ADF Statistic (Differenced):', result_diff[0])\n",
    "print('p-value (Differenced):', result_diff[1])\n",
    "for key, value in result_diff[4].items():\n",
    "    print('Critical Values (Differenced):')\n",
    "    print(f' {key}, {value}')\n",
    "\n",
    "# 차분 시계열 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(diff_series, label='Differenced Time Series')\n",
    "plt.title('Stationary Time Series After Differencing')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "(Figure 4.4와 4.5)\n",
    "\n",
    "```\n",
    "ADF Statistic: –1.3583317659818992\n",
    "p-value: 0.6020814791099098\n",
    "Critical Values:\n",
    "   1%, –3.498198082189098\n",
    "   5%, –2.891208211860468\n",
    "   10%, –2.5825959973472097\n",
    "ADF Statistic (Differenced): –10.008881137130237\n",
    "p-value (Differenced): 1.800687720719554e-17\n",
    "Critical Values (Differenced):\n",
    "   1%, –3.4989097606014496\n",
    "   5%, –2.891516256916761\n",
    "   10%, –2.5827604414827157\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_04_04.png\" width=800>\n",
    "\n",
    "Figure 4.4 비정상 시계열 예시\n",
    "\n",
    "<img src=\"./images/fig_04_05.png\" width=800>\n",
    "\n",
    "Figure 4.5 차분 후 정상 시계열 예시\n",
    "\n",
    "**분수 차분(Fractional Differentiation)**\n",
    "\n",
    "금융에서는 과도한 차분을 피하면서 신호의 메모리를 유지하기 위해 Lopez de Prado (2018, pp. 79–84)가 제안한 **분수 차분 기법**이 권장됩니다. 이 방법은 정상성과 예측 정보 유지 간의 균형을 추구합니다.\n",
    "\n",
    "아래는 분수 차분 기법을 활용한 ADF 테스트 예시입니다:\n",
    "\n",
    "```python\n",
    "# 중략 - 위와 동일한 코드 (get_weights_ffd, frac_diff_ffd, ffd 함수 정의 포함)\n",
    "\n",
    "# 시계열 생성\n",
    "np.random.seed(42)\n",
    "time_series = np.random.randn(100).cumsum()\n",
    "\n",
    "# ADF 테스트\n",
    "result = adfuller(time_series)\n",
    "print('ADF Statistic:', result[0])\n",
    "print('p-value:', result[1])\n",
    "for key, value in result[4].items():\n",
    "    print('Critical Values:')\n",
    "    print(f'   {key}, {value}')\n",
    "\n",
    "# 시계열 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(time_series, label='Original Time Series')\n",
    "plt.title('Non-Stationary Time Series')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 분수 차분 적용\n",
    "diff_series = ffd(pd.Series(time_series, name='time_series'))\n",
    "\n",
    "# 차분 후 ADF 테스트\n",
    "result_diff = adfuller(diff_series)\n",
    "print('ADF Statistic (Differenced):', result_diff[0])\n",
    "print('p-value (Differenced):', result_diff[1])\n",
    "for key, value in result_diff[4].items():\n",
    "    print('Critical Values (Differenced):')\n",
    "    print(f'   {key}, {value}')\n",
    "\n",
    "# 차분 시계열 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(diff_series, label='Differenced Time Series')\n",
    "plt.title('Stationary Time Series After Differencing')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과:\n",
    "\n",
    "```\n",
    "ADF Statistic: –1.3583317659818992\n",
    "p-value: 0.6020814791099098\n",
    "Critical Values:\n",
    "   1%, –3.498198082189098\n",
    "   5%, –2.891208211860468\n",
    "   10%, –2.5825959973472097\n",
    "ADF Statistic (Differenced): –3.614042461855659\n",
    "p-value (Differenced): 0.0054981717565326035\n",
    "Critical Values (Differenced):\n",
    "   1%, –3.506057133647011\n",
    "   5%, –2.8946066061911946\n",
    "   10%, –2.5844100201994697\n",
    "```\n",
    "\n",
    "ADF Statistic이 모든 임계값보다 작기 때문에 해당 시계열은 1%, 5%, 10% 수준에서 모두 정상성(stationarity)을 만족합니다.\n",
    "\n",
    "<img src=\"./images/fig_04_06.png\" width=800>\n",
    "\n",
    "Figure 4.6 원본 비정상 시계열 (분수 차분 전)\n",
    "\n",
    "<img src=\"./images/fig_04_07.png\" width=800>\n",
    "\n",
    "Figure 4.7 분수 차분을 통해 얻어진 정상 시계열\n",
    "\n",
    "**Engle-Granger 테스트를 활용한 공적분 시계열 식별 (Identification of Cointegrated Time Series with EngleGranger Test)**\n",
    "\n",
    "두 개 이상의 비정상 시계열이 선형 결합을 통해 정상 시계열을 형성할 수 있다면, 이는 안정적인 장기 균형 관계를 의미하며 공적분되었다고 말합니다.\n",
    "\n",
    "Engle-Granger 테스트는 공적분을 검정하기 위한 널리 사용되는 방법으로, 평균회귀 행동을 보이는 자산 쌍을 식별하여 절대 가격이 아닌 두 자산 간의 상대적 움직임에 베팅할 수 있도록 합니다.\n",
    "\n",
    "다음은 공적분된 두 비정상 시계열의 예제입니다 (Figure 4.8).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시계열 데이터 생성\n",
    "np.random.seed(42)\n",
    "n = 100\n",
    "time = np.arange(n)\n",
    "\n",
    "# 두 개의 비정상 시계열(랜덤워크) 생성\n",
    "asset1 = np.cumsum(np.random.randn(n)) + 41\n",
    "asset2 = asset1 + np.random.randn(n)\n",
    "\n",
    "# DataFrame 생성\n",
    "data = pd.DataFrame({'asset1': asset1, 'asset2':asset2})\n",
    "# ADF 테스트 함수\n",
    "def adf_test(series, name):\n",
    "    result = adfuller(series)\n",
    "    print(f'ADF Statistic for {name}: {result[0]}')\n",
    "    print(f'p-value for {name}: {result[1]}')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'Critical Value {key}: {value}')\n",
    "    print('\\n')\n",
    "\n",
    "# 1단계: 각 시계열의 정상성 테스트\n",
    "print(\"ADF Test for asset1:\")\n",
    "adf_test(data['asset1'], 'asset1')\n",
    "print(\"ADF Test for asset2:\")\n",
    "adf_test(data['asset2'], 'asset2')\n",
    "\n",
    "# 2단계: Engle-Granger 공적분 테스트 수행\n",
    "score, pvalue, _ = coint(data['asset1'], data['asset2'])\n",
    "print(f'Engle-Granger Cointegration Test score:{score}')\n",
    "print(f'Engle-Granger Cointegration Test p-value:{pvalue}\\n')\n",
    "\n",
    "# 3단계: 시계열 및 스프레드 시각화\n",
    "data['spread'] = data['asset1'] - data['asset2']\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(data['asset1'], label='Asset 1')\n",
    "plt.plot(data['asset2'], label='Asset 2')\n",
    "plt.legend()\n",
    "plt.title('Non-Stationary Time Series')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(data['spread'], label='Spread (Asset 1 - Asset2)')\n",
    "plt.legend()\n",
    "plt.title('Spread (Should be Stationary)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4단계: 스프레드의 정상성 테스트\n",
    "print(\"ADF Test for spread:\")\n",
    "adf_test(data['spread'], 'spread')\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.8)\n",
    "및\n",
    "\n",
    "```\n",
    "ADF Test for asset1:\n",
    "ADF Statistic for asset1: -1.3583317659819\n",
    "p-value for asset1: 0.6020814791099095\n",
    "Critical Value 1\\%: -3.498198082189098\n",
    "Critical Value 5%: -2.891208211860468\n",
    "Critical Value 10%: -2.5825959973472097\n",
    "\n",
    "ADF Test for asset2:\n",
    "ADF Statistic for asset2: -1.642378142665797\n",
    "p-value for asset2: 0.4610184160167067\n",
    "Critical Value 1%: -3.5019123847798657\n",
    "Critical Value 5%: -2.892815255482889\n",
    "Critical Value 10%: -2.583453861475781\n",
    "Engle-Granger Cointegration Test score: 10.546923889518597\n",
    "Engle-Granger Cointegration Test p-value: 1.0672395686754609e-17\n",
    "\n",
    "ADF Test for spread:\n",
    "ADF Statistic for spread: -10.875458876565842\n",
    "p-value for spread: 1.3352313844030579e-19\n",
    "Critical Value 1%: -3.498198082189098\n",
    "Critical Value 5%: -2.891208211860468\n",
    "Critical Value 10%: -2.5825959973472097\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_04_08.png\" width=800>\n",
    "\n",
    "Figure 4.8 공적분된 두 비정상 자산은 정상적인 스프레드를 가진다.\n",
    "\n",
    "시계열이 정상적인지 여부를 결정하기 위해서는 ADF 테스트 통계량을 임계값과 비교하고 p-value를 확인해야 합니다.\n",
    "\n",
    "임계값은 다음과 같습니다:\n",
    "\n",
    "* Critical Value 1%: -3.4996365338407074\n",
    "* Critical Value 5%: -2.8918307730370025\n",
    "* Critical Value 10%: -2.5829283377617176\n",
    "\n",
    "ADF 통계량이 임계값보다 작으면, 귀무가설(두 시계열이 공적분되지 않았다는 가설)을 기각하고, 시계열이 공적분되었다고 결론지을 수 있습니다.\n",
    "\n",
    "asset1의 ADF 통계량은 -1.3583317659819로 모든 임계값(1%, 5%, 10%)보다 크기 때문에 확실히 비정상입니다.\n",
    "asset2의 ADF 통계량은 -1.642378142665797로 역시 모든 임계값보다 커서 비정상입니다.\n",
    "그러나 스프레드의 ADF 통계량은 -10.875458876565842로 임계값보다 작아 해당 시계열이 공적분되었음을 나타냅니다.\n",
    "\n",
    "요약하면, 개별 자산은 비정상이지만 스프레드는 정상이며, 이는 페어 트레이딩에 적합하다는 뜻입니다.\n",
    "\n",
    "다음은 스프레드의 p-value (1.3352313844030579e-19)를 기준으로 p-value의 개념을 설명합니다.\n",
    "귀무가설이 참이라는 가정 하에, p-value는 관측된 결과보다 더 극단적인 결과를 얻을 확률입니다.\n",
    "p-value가 0.05보다 작으면 귀무가설을 기각할 수 있는 강한 증거이며, 본 예제에서는 시계열이 공적분되었음을 시사합니다.\n",
    "p-value가 0.05보다 크면 귀무가설을 기각하지 못하며, 이 경우 시계열은 공적분되지 않았음을 의미합니다.\n",
    "\n",
    "추가적으로, 시계열의 장기 메모리 특성을 0\\~1 사이의 값으로 측정하는 Hurst 계수는 공적분된 시계열의 특성을 파악하는 데 유용합니다:\n",
    "\n",
    "* \\$\\mathrm{H}<0.5\\$ : 시계열이 평균회귀 성향을 가짐\n",
    "* \\$\\mathrm{H}=0.5\\$ : 무작위 보행(Random Walk)과 같은 성향\n",
    "* \\$\\mathrm{H}>0.5\\$ : 강한 추세를 가짐\n",
    "\n",
    "스프레드의 Hurst 계수가 0.5보다 상당히 낮을 경우, 평균회귀 성향이 강해 페어 트레이딩에 이상적입니다.\n",
    "(시장 중립 전략으로, 성과가 낮은 자산에 롱 포지션을, 성과가 높은 자산에 숏 포지션을 취해 두 자산이 균형 상태로 수렴할 때 수익을 기대함)\n",
    "\n",
    "다음은 공적분된 두 시계열의 간단한 예제를 통해 Hurst 계수를 설명합니다.\n",
    "(Hurst 지수 계산에 로그가 포함되므로, 음수 및 0 값은 제거합니다) (Figure 4.9)\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller, coint\n",
    "import matplotlib.pyplot as plt\n",
    "from hurst import compute_Hc\n",
    "\n",
    "# 공적분된 시계열 데이터 생성\n",
    "np.random.seed(42)\n",
    "n = 1000 # 데이터 포인트 수\n",
    "time = np.arange(n)\n",
    "\n",
    "# 공통 확률 추세 생성\n",
    "trend = np.cumsum(np.random.randn(n))\n",
    "# 노이즈를 더해 공적분된 시계열 생성\n",
    "asset1 = trend + 0.3 * np.random.randn(n)\n",
    "asset2 = trend + 0.1 * np.random.randn(n)\n",
    "\n",
    "# DataFrame 생성\n",
    "data = pd.DataFrame({'asset1': asset1, 'asset2':asset2})\n",
    "\n",
    "# ADF 테스트 함수\n",
    "def adf_test(series, name):\n",
    "    result = adfuller(series)\n",
    "    print(f'ADF Statistic for {name}: {result[0]}')\n",
    "    print(f'p-value for {name}: {result[1]}')\n",
    "    for key, value in result[4].items():\n",
    "        print(f'Critical Value {key}: {value}')\n",
    "    print('\\n')\n",
    "\n",
    "# 각 시계열의 정상성 테스트\n",
    "print(\"ADF Test for asset1:\")\n",
    "adf_test(data['asset1'], 'asset1')\n",
    "print(\"ADF Test for asset2:\")\n",
    "adf_test(data['asset2'], 'asset2')\n",
    "\n",
    "# Engle-Granger 공적분 테스트 수행\n",
    "score, pvalue, _ = count(data['asset1'], data['asset2'])\n",
    "print(f'Engle-Granger Cointegration Test score:{score}')\n",
    "print(f'Engle-Granger Cointegration Test p-value:{pvalue}\\n')\n",
    "\n",
    "# 스프레드 계산\n",
    "data['spread'] = data['asset1'] - data['asset2']\n",
    "\n",
    "# 4단계: 스프레드의 정상성 테스트\n",
    "print(\"ADF Test for spread:\")\n",
    "adf_test(data['spread'], 'spread')\n",
    "\n",
    "# Hurst 계산을 위한 0 이하 값 제거\n",
    "spread_non_zero = data['spread'][data['spread'] > 0]\n",
    "if len(spread_non_zero) < len(data['spread']):\n",
    "    print(\"Warning: Hurst 계산을 위해 스프레드에서 0 이하 값이 제거되었습니다.\")\n",
    "\n",
    "# 유효성 검사 후 Hurst 지수 계산\n",
    "if len(spread_non_zero) > 0 and (spread_non_zero <= 0).sum() == 0:\n",
    "    # Hurst 지수 계산\n",
    "    H, c, data_hurst = compute_Hc(spread_non_zero, kind='price', simplified=True)\n",
    "    print(f'Hurst Exponent for spread: {H}')\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(data['asset1'], label='Asset 1')\n",
    "    plt.plot(data['asset2'], label='Asset 2')\n",
    "    plt.legend()\n",
    "    plt.title('Cointegrated Time Series')\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(data['spread'], label='Spread (Asset 1 - Asset 2)')\n",
    "    plt.legend()\n",
    "    plt.title('Spread (Should be Stationary)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Error: 스프레드가 유효하지 않거나 0 이하 값 제거 후 비어 있습니다.\")\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.9)\n",
    "및\n",
    "\n",
    "```\n",
    "ADF Test for asset1:\n",
    "ADF Statistic for asset1: -0.9841906190427494\n",
    "p-value for asset1: 0.7589866498633684\n",
    "Critical Value 1%: -3.4369193380671\n",
    "Critical Value 5%: -2.864440383452517\n",
    "Critical Value 10%: -2.56831430323573\n",
    "\n",
    "ADF Test for asset2:\n",
    "ADF Statistic for asset2: -1.0039798833431093\n",
    "p-value for asset2: 0.7518099637843659\n",
    "Critical Value 1%: -3.4369127451400474\n",
    "Critical Value 5%: -2.864437475834273\n",
    "Critical Value 10%: -2.568312754566378\n",
    "\n",
    "Engle-Granger Cointegration Test score: -\n",
    "31.604955223815125\n",
    "\n",
    "Engle-Granger Cointegration Test p-value: 0.0\n",
    "ADF Test for spread:\n",
    "ADF Statistic for spread: -31.570817098575255\n",
    "p-value for spread: 0.0\n",
    "Critical Value 1\\%: -3.4369127451400474\n",
    "Critical Value 5\\%: -2.864437475834273\n",
    "Critical Value 10\\%: -2.568312754566378\n",
    "\n",
    "Warning: Non-positive values in spread were removed for\n",
    "Hurst calculation.\n",
    "Hurst Exponent for spread: 0.3099860341085018\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_04_09.png\" width=800>\n",
    "\n",
    "Figure 4.9 공적분된 두 자산과 정상적인 스프레드\n",
    "Hurst 지수가 0.3으로, 해당 자산 쌍은 페어 트레이딩에 이상적입니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특성 선택 (Feature Selection)\n",
    "\n",
    "특성 선택의 주요 목표는 예측 모델의 성능에 기여하는 가장 관련성이 높은 특성을 식별하고, 관련 없는 특성을 제거하여 과적합을 줄이고 모델의 해석력을 향상시키는 것입니다.\n",
    "\n",
    "다음은 대표적인 네 가지 특성 선택 방법입니다: 상관 분석, 특성 중요도 분석, 자동 특성 식별, 주성분 분석(PCA)을 통한 차원 축소.\n",
    "\n",
    "**상관 분석 (Correlation Analysis)**\n",
    "\n",
    "상관 분석은 대상 변수와 높은 상관관계를 가진 특성, 그리고 서로 간에 높은 상관관계를 가지는 특성을 식별하는 데 유용합니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상관관계를 가진 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "size = 100\n",
    "feature1 = np.random.randn(size)\n",
    "feature2 = feature1 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "feature3 = np.random.randn(size)\n",
    "target = feature1 * 0.5 + np.random.randn(size) * 0.1 # Target과 높은 상관관계\n",
    "data = {\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 상관 행렬 계산\n",
    "corr_matrix = df.corr()\n",
    "\n",
    "# 상관 행렬 출력\n",
    "print(corr_matrix)\n",
    "\n",
    "# 히트맵 시각화\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.10)\n",
    "\n",
    "<img src=\"./images/fig_04_10.png\" width=800>\n",
    "\n",
    "Figure 4.10 Target과 요인들 간의 상관 행렬\n",
    "\n",
    "**특성 중요도 분석 (Feature Importance Analysis)**\n",
    "\n",
    "랜덤 포레스트와 같은 트리 기반 머신러닝 모델은 각 특성이 대상 변수 예측에 얼마나 기여하는지를 자연스럽게 판단하므로, 특성 중요도 분석에 자주 사용됩니다.\n",
    "상관관계가 높은 특성들을 제외하고 분석하는 것이 좋기 때문에, 먼저 상관계수가 \\$>0.9\\$ 인 특성은 제거한 후 특성 중요도 분석을 진행합니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상관관계를 가진 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "size = 100\n",
    "feature1 = np.random.randn(size)\n",
    "feature2 = feature1 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "feature3 = np.random.randn(size)\n",
    "target = feature1 * 0.5 + np.random.randn(size) * 0.1 # Target과 높은 상관관계\n",
    "data = {\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# 상관관계 높은 특성 제거\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "# 랜덤 포레스트 모델 학습\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_reduced, y)\n",
    "\n",
    "# 특성 중요도 추출\n",
    "importances = model.feature_importances_\n",
    "feature_names = X_reduced.columns\n",
    "importance_df = pd.DataFrame({'Feature': feature_names,'Importance': importances})\n",
    "\n",
    "# 특성 중요도 출력\n",
    "print(importance_df)\n",
    "\n",
    "# 특성 중요도 시각화\n",
    "importance_df.sort_values(by='Importance',\n",
    "ascending=False).plot(kind='bar', x='Feature',\n",
    "y='Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.11)\n",
    "\n",
    "|    | Feature  | Importance |\n",
    "| :- | :------- | :--------- |\n",
    "| 0  | Feature1 | 0.984778   |\n",
    "| 1  | Feature3 | 0.015222   |\n",
    "\n",
    "<img src=\"./images/fig_04_11.png\" width=800>\n",
    "\n",
    "Figure 4.11 Feature 1과 3의 특성 중요도 플롯. Feature 3은 모델을 단순화하고 데이터셋 차원을 줄이기 위해 제거될 수 있습니다.\n",
    "\n",
    "**특성의 자동 식별 (Auto-identification of Features)**\n",
    "\n",
    "특성의 자동 식별은 알고리즘과 통계적 방법을 사용하여 주어진 예측 모델에 가장 관련 있는 특성을 자동으로 선택하는 것을 의미합니다.  \n",
    "대표적인 기법으로는 재귀적 특성 제거(Recursive Feature Elimination, RFE)와 `sklearn`의 `SelectFromModel`이 있습니다.\n",
    "\n",
    "특성 간 상관관계가 높은 경우, RFE는 중요하지 않은 특성을 반복적으로 제거하고 모델을 다시 학습하는 방식이지만, 서로 상관관계가 높은 여러 특성들을 각각 중요하다고 판단할 수 있으므로, 이러한 특성들을 먼저 제거해 줍니다.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상관관계를 가진 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "size = 100\n",
    "feature1 = np.random.randn(size)\n",
    "feature2 = feature1 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "feature3 = np.random.randn(size)\n",
    "target = feature1 * 0.5 + np.random.randn(size) * 0.1 # Target과 높은 상관관계\n",
    "data = {\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# 상관관계 높은 특성 제거\n",
    "corr_matrix = X.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "X_reduced = X.drop(columns=to_drop)\n",
    "\n",
    "# 재귀적 특성 제거(RFE) 적용\n",
    "model = RandomForestRegressor()\n",
    "rfe = RFE(estimator=model, n_features_to_select=2)\n",
    "rfe.fit(X_reduced, y)\n",
    "\n",
    "# 선택된 특성 출력\n",
    "selected_features = X_reduced.columns[rfe.support_]\n",
    "print(f'Selected Features: {selected_features}')\n",
    "\n",
    "# 선택된 특성 시각화\n",
    "importance_df = pd.DataFrame({'Feature': X_reduced.columns, 'Importance': rfe.support_.astype(int)})\n",
    "importance_df.plot(kind='bar', x='Feature', y='Importance')\n",
    "plt.title('Auto-Identification of Features')\n",
    "\n",
    "# x축 라벨 여백 확보\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.12)\n",
    "\n",
    "```\n",
    "Selected Features: Index(['Feature1', 'Feature3'], dtype='object')\n",
    "```\n",
    "\n",
    "<img src=\"./images/fig_04_12.png\" width=800>\n",
    "\n",
    "Figure 4.12 재귀적 특성 제거(RFE)로 선택된 Feature1과 Feature3의 중요도 플롯\n",
    "\n",
    "**차원 축소 / 주성분 분석 (Dimensionality Reduction/Principal Component Analysis)**\n",
    "\n",
    "차원이란 데이터셋의 특성(feature) 수를 의미합니다.\n",
    "데이터셋에 포함된 특성 수가 많을수록 계산 비용과 메모리 사용량이 증가하고, 데이터 내에서 유의미한 패턴을 찾는 것이 어려워지며, 모델의 과적합 가능성이 높아집니다.\n",
    "\n",
    "주성분 분석(PCA)은 많은 특성을 가진 복잡한 데이터셋을 분석할 때 데이터를 더 작고 새로운 특성 집합으로 변환하여 대부분의 변동성을 보존하도록 하는 통계 기법입니다.\n",
    "\n",
    "새로 생성된 특성들은 주성분(principal components)이라고 하며, 원래 특성의 선형 조합이고 서로 상관관계가 없습니다.\n",
    "이들은 설명하는 분산의 크기 순으로 정렬되어 있으며, 첫 번째 주성분이 가장 많은 분산을 설명합니다.\n",
    "\n",
    "PCA를 적용하기 전에는 모든 특성이 주성분에 동등하게 기여하도록 **표준화(Standardization)** 가 필수입니다.\n",
    "\n",
    "금융에서는 PCA를 리스크 관리, 포트폴리오 최적화, 대규모 데이터셋의 복잡도 감소 등에 활용합니다.\n",
    "\n",
    "PCA 적용 단계는 다음과 같습니다:\n",
    "\n",
    "1. 각 특성 쌍 간의 공분산을 계산하여 **공분산 행렬** 생성\n",
    "2. 공분산 행렬의 **고유벡터**(주성분 방향)와 **고유값**(각 주성분이 설명하는 분산의 양) 계산\n",
    "3. 고유값을 내림차순으로 정렬한 후, 설명하려는 전체 분산의 비율(예: \\$90%\\$)을 만족하는 상위 K개의 고유벡터 선택\n",
    "4. 선택된 상위 K개의 고유벡터로 구성된 **특성 행렬(feature matrix)** 생성\n",
    "5. 원본 표준화 데이터를 특성 행렬에 곱하여 새로운 **주성분 공간**으로 투영\n",
    "\n",
    "`sklearn`의 PCA를 사용한 Python 구현 예시는 다음과 같습니다:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 상관관계를 가진 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "size = 100\n",
    "feature1 = np.random.randn(size)\n",
    "feature2 = feature1 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "feature3 = np.random.randn(size)\n",
    "target = feature1 * 0.5 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "data = {\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "X = df.drop('Target', axis=1)\n",
    "\n",
    "# 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# PCA 적용\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_standardized)\n",
    "\n",
    "# 주성분 DataFrame 생성\n",
    "pca_df = pd.DataFrame(data=principal_components,\n",
    "columns=['Principal Component 1', 'Principal Component2'])\n",
    "\n",
    "# 설명된 분산 비율 출력\n",
    "print(f'Explained variance ratio:{pca.explained_variance_ratio_}')\n",
    "\n",
    "# 주성분 시각화\n",
    "plt.scatter(pca_df['Principal Component 1'],\n",
    "pca_df['Principal Component 2'])\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Principal Component Analysis') \n",
    "plt.show()\n",
    "```\n",
    "\n",
    "출력 결과 (Figure 4.13)\n",
    "\n",
    "<img src=\"./images/fig_04_13.png\" width=800>\n",
    "\n",
    "Figure 4.13 PCA는 세 개의 원본 특성으로 구성된 행렬을 분산을 최대한 보존하면서 두 개의 새로운 차원으로 변환하였다.\n",
    "\n",
    "산점도에서 각 점은 원본 데이터셋의 샘플 하나를 의미하며, 주성분 1과 2로 정의된 평면에 투영된 것입니다.\n",
    "각 축을 따라 퍼져 있는 정도는 해당 주성분이 얼마나 많은 분산을 설명하고 있는지를 나타냅니다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋을 학습, 테스트, (선택적으로) 검증 세트로 분할하기 (Splitting of Dataset into Training, Testing, and Possibly Validation Sets)\n",
    "\n",
    "일반적으로 머신러닝 모델을 개발하고 평가할 때, 데이터셋을 학습, 테스트, 그리고 (선택적으로) 검증 세트로 분리하는 것이 권장됩니다:\n",
    "\n",
    "1. **학습 세트(Training Set)**: 이 데이터 하위 집합은 모델을 학습시키는 데 사용됩니다. 모델은 이 데이터를 기반으로 패턴과 관계를 학습합니다.  \n",
    "2. **테스트 세트(Testing Set)**: 모델 학습 후에는 이 세트를 통해 성능을 평가하고, 보지 못한 새로운 데이터에 대한 일반화 능력을 측정합니다. 과적합(overfitting)이나 과소적합(underfitting) 문제를 식별할 수 있게 도와줍니다.  \n",
    "   - 과적합은 모델이 훈련 데이터를 너무 잘 외워버려 데이터의 노이즈나 이상값까지 학습하고, 새로운 데이터에 대해 일반화가 잘 되지 않는 오류입니다.  \n",
    "   - 반면 과소적합은 모델이 너무 단순하여 데이터 내의 패턴을 제대로 학습하지 못하고, 훈련 데이터와 테스트 데이터 모두에서 성능이 저조한 오류입니다.  \n",
    "3. **검증 세트(Validation Set, 선택적)**: 훈련 중 하이퍼파라미터 튜닝에 사용됩니다. 하이퍼파라미터는 학습 이전에 사전에 설정해야 하는 머신러닝 모델의 설정값입니다.  \n",
    "   - 학습 데이터에서 학습되는 모델 파라미터와 달리, 하이퍼파라미터는 사전에 정의되며, 검증 세트를 통해 과적합을 방지하고 학습 중 적절성을 확인합니다.\n",
    "\n",
    "**데이터를 분할하는 방법 (How to Split Your Data)**\n",
    "\n",
    "1. **학습/테스트 분할**\n",
    "   - 일반적으로 데이터의 $70-80 \\%$ 를 학습에, $20-30 \\%$ 를 테스트에 사용합니다.  \n",
    "   - `sklearn`의 `train_test_split` 함수를 사용하면 쉽게 분할할 수 있습니다.\n",
    "\n",
    "2. **학습/검증/테스트 분할**\n",
    "   - 검증 세트를 사용할 경우, 일반적으로 데이터의 $60 \\%$ 는 학습, $20 \\%$ 는 검증, $20 \\%$ 는 테스트에 사용합니다.\n",
    "\n",
    "다음은 `sklearn`을 사용한 예시입니다:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "np.random.seed(42)\n",
    "size = 100\n",
    "feature1 = np.random.randn(size)\n",
    "feature2 = feature1 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "feature3 = np.random.randn(size)\n",
    "target = feature1 * 0.5 + np.random.randn(size) * 0.1 # Feature1과 높은 상관관계\n",
    "data = {\n",
    "    'Feature1': feature1,\n",
    "    'Feature2': feature2,\n",
    "    'Feature3': feature3,\n",
    "    'Target': target\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 학습/테스트 세트 분할\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 분할된 데이터 형태 출력\n",
    "print(f'Training features shape: {X_train.shape}')\n",
    "print(f'Training target shape: {y_train.shape}')\n",
    "print(f'Testing features shape: {X_test.shape}')\n",
    "```\n",
    "\n",
    "출력 결과:\n",
    "\n",
    "```\n",
    "Training features shape: (80,3)\n",
    "Training target shape: (80,)\n",
    "Testing features shape: (20, 3)\n",
    "Testing target shape: (20,)\n",
    "```\n",
    "\n",
    "`sklearn`의 `train_test_split` 함수\n",
    "\n",
    "```python\n",
    "sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "```\n",
    "\n",
    "이 함수는 다음과 같은 파라미터를 가집니다:\n",
    "\n",
    "1. **arrays**\n",
    "\n",
    "   * 분할하고자 하는 입력 데이터 (예: X, y)\n",
    "\n",
    "2. **test\\_size**\n",
    "\n",
    "   * 테스트 세트에 포함할 비율 또는 샘플 수 (float 또는 int). `None`이면 `train_size` 보완 비율로 설정됩니다.\n",
    "\n",
    "3. **train\\_size**\n",
    "\n",
    "   * 학습 세트에 포함할 비율 또는 샘플 수 (float 또는 int). `None`이면 `test_size` 보완 비율로 설정됩니다.\n",
    "\n",
    "4. **random\\_state**\n",
    "\n",
    "   * 데이터 분할 시 재현성을 위해 셔플 순서를 고정할 수 있습니다.\n",
    "\n",
    "5. **shuffle**\n",
    "\n",
    "   * 분할 전에 데이터를 셔플할지 여부 (`True`가 기본)\n",
    "\n",
    "6. **stratify**\n",
    "\n",
    "   * 이 인자를 설정하면 지정된 클래스 라벨을 기준으로 비율에 맞춰 분할합니다 (분류 문제에서 유용)\n",
    "\n",
    "데이터셋이 작거나 불균형한 경우(한 클래스가 다른 클래스보다 월등히 많은 경우), 혹은 모델의 강건함을 더 잘 평가하고 싶다면 **교차검증(Cross-validation)** 을 사용하는 것이 좋습니다.\n",
    "\n",
    "**교차검증**은 데이터를 여러 개의 부분집합(\"폴드\")으로 나누고, 각 조합을 바꿔가며 학습/평가를 반복하는 통계적 방법입니다.\n",
    "\n",
    "가장 일반적인 교차검증 방식은 **k-겹 교차검증(k-fold cross-validation)** 입니다.\n",
    "\n",
    "* 데이터셋을 k개의 동일한 크기의 폴드로 나눈 후,\n",
    "* 그 중 k-1개의 폴드로 학습하고 나머지 1개의 폴드로 평가\n",
    "* 이 과정을 k번 반복하며 모든 폴드가 한 번씩 테스트에 사용됨\n",
    "* 최종 모델 성능은 k번의 평균 점수로 평가됨\n",
    "\n",
    "이 방식은 단일 테스트 세트보다 **모델 성능에 대한 더 신뢰할 수 있는 추정치를 제공**하지만, 계산 비용이 더 큽니다.\n",
    "\n",
    "다음은 UCI 머신러닝 저장소의 와인 데이터셋을 예시로, 학습/테스트 분할 방식과 k-겹 교차검증을 비교한 것입니다.\n",
    "이 데이터는 이탈리아에서 재배된 세 종류 포도 품종에 대한 화학적 분석을 기반으로 합니다.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import cross_val_score,train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 데이터 로드\n",
    "data = load_wine()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# 모델 초기화\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 5겹 교차검증 수행\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# 학습/테스트 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 테스트 예측 및 정확도 평가\n",
    "y_pred = model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Train-test split accuracy:\", test_accuracy)\n",
    "\n",
    "# 결과 비교 출력\n",
    "print(\"\\nComparison of Results:\")\n",
    "print(f\"Mean cross-validation score:{cv_scores.mean():.4f}\")\n",
    "print(f\"Train-test split accuracy: {test_accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "출력 결과:\n",
    "\n",
    "```\n",
    "Cross-validation scores: [0.97222222 0.94444444, 0.97222222 0.97142857 1.]\n",
    "Mean cross-validation score: 0.9720634920634922\n",
    "Train-test split accuracy: 1.0\n",
    "\n",
    "Comparison of Results:\n",
    "Mean cross-validation score: 0.9721\n",
    "Train-test split accuracy: 1.0000\n",
    "```\n",
    "\n",
    "이 결과는 해당 데이터셋에서 모델이 잘 작동하고 있음을 보여주며,\n",
    "train-test split에서 정확도가 완벽하게 나온 것은 **과적합 가능성**이 존재함을 시사합니다.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
